{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a6b6c6-ecfc-4923-8b2f-7c1bb9d48125",
   "metadata": {},
   "source": [
    "# DEMO for empathy classification\n",
    "\n",
    "In this demo, we classify the empathy in text exchanges. \n",
    "\n",
    "We provide the most supported pattern for the classification of the exchange. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367a65a-8f57-4dcd-8e31-f8b45938155d",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c241feda-8334-4a31-98e6-fc64620acb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PBC4cip import PBC4cip\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import database_processing_package as data_processer\n",
    "import exchange_processing as exchange_processer\n",
    "from classifiers.course_grained_emotion import emotion_reductor as em_red\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f28688-b0f0-42c8-b1e8-f39fe6b0d959",
   "metadata": {},
   "source": [
    "### Load main classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b8dd876-8b08-402b-916c-5d1386a271a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for the model: \n",
      "exchange_number s_negative s_neutral s_positive l_negative l_neutral l_positive predictions_ER predictions_IP predictions_EX valence_speaker arousal_speaker dominance_speaker valence_listener arousal_listener dominance_listener s_word_len l_word_len agreeing acknowledging encouraging consoling sympathizing suggesting questioning wishing neutral mimicry "
     ]
    }
   ],
   "source": [
    "#Relevant directories\n",
    "current_dir = os.getcwd() #get directory of the repository\n",
    "\n",
    "#Database\n",
    "database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv'\n",
    "test_database_dir = '/processed_databases/EmpatheticExchanges/test.csv'\n",
    "database = pd.read_csv(current_dir + database_dir)\n",
    "test_db = pd.read_csv(current_dir + test_database_dir)\n",
    "\n",
    "#Select an appropriate classification model in the Experiments folder\n",
    "#number_of_model =90 #The number of the experiment for the model of interest\n",
    "\n",
    "number_of_model = 70 #Best performing model without exchange number (broader pattern recognition)\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(number_of_model) + '/' + 'trained_pbc4cip.sav'\n",
    "pbc = pickle.load(open(model_directory, 'rb'))\n",
    "\n",
    "#Select features relevant for the model\n",
    "att_lst = [attribute[0] for attribute in pbc.dataset.Attributes]\n",
    "print('Features for the model: ')\n",
    "for attribute in att_lst:\n",
    "    print(attribute, end = ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8d488-f425-473a-9919-cd2878c5e726",
   "metadata": {},
   "source": [
    "### Load supplementary classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a94c89-a47c-4ed2-be22-0f8fcac352d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_array, model_components = exchange_processer.load_supplementary_classifiers(att_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81cf7a",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0cf99e-4fa8-425a-a110-eec158710650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21897c3-6718-4f34-b5f0-d4825776ec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'I hate when my wife and son are away from me', \"I get that you're feeling bad but do not let it get to you. I'm sure you'll be extra happy once they are here\",model_components)\n",
    "recommendation = exchange_processer.get_recommentation(pbc, processed_exchange,'listener')\n",
    "judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,'I hate when my wife and son are away from me', \"I get that you're feeling bad but do not let it get to you. I'm sure you'll be extra happy once they are here\",model_components,\"listener\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4012c7-8096-4640-876a-dd356e1af0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,'I hate when my wife and son are away from me', \"Aww that is sweet You are a good dad\",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,'My little cousin  was nice and gave me a present!', \"Aww cool! was the ocasion special?\",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadecuate\", \"Yeah, it really sucks loneliness is no easy thing to go though\",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadecuate\", \"Yeah, you are \",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadecuate\", \"Oh yeah, it bothers me a lot too! \",model_components,\"listener\")\n",
    "\n",
    "\n",
    "\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Fucking really? That sucks so much I would be pissed! \",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"That's what you get lmao! \",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Aww man, that's not ideal Did you get the plates?\",model_components,\"listener\")\n",
    "#judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Was it a bad accident?\",model_components,\"listener\")\n",
    "\n",
    "\n",
    "judge_exchange = exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Was it a bad accident?\",model_components,\"listener\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208ce61-4d17-460a-9e4e-09dd62c16f29",
   "metadata": {},
   "source": [
    "print(\"First exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"i loved taking care of my sisters pet\", \"Huh, is that so\",model_components, 'listener')\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"i loved taking care of my sisters pet\", \"It's cool that you loved that\",model_components, 'listener')\n",
    "print(\"Second exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Yeah! I have loved animals since then especially dogs\", \"Dogs are very cute. Cats too\",model_components, \"listener\")\n",
    "print(\"Third exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Oh my gosh yes. Cats are so fluffy and cuddly\", \"They are! I love petting them\", model_components,\"listener\")\n",
    "print(\"Fourth exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"You know i really enjoy having pets they bring a new life into an empty feeling house\", \"Yes I only have one cat. How about you?\",model_components, \"listener\")\n",
    "print(\"Fifth exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"we have a cat, a dog, a bunny, and a betta fish!\", \"Those are many pets\", model_components, \"listener\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"we have a cat, a dog, a bunny, and a betta fish!\", \"Those are many pets, how do you manage?\",model_components, \"listener\")\n",
    "print(\"Sixth exchange\")\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"It is a lot of work. But their little faces are so worth it\", \"Yes they are I bet you feel proud\",model_components, \"listener\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "578112f1-1657-4bef-9ddd-a71a473ecf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 1/3\n",
      "Low empathy detected!\n",
      "interjection by Haru: Suggestion: Respond with sympathy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 1/3\n",
      "Low empathy detected!\n",
      "interjection by Haru: Suggestion: My sensors say that we should encourage them about their feelings listener\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy prediction for listener: 1/3\n",
      "Low empathy detected!\n",
      "interjection by Haru: Suggestion: My sensors say that we should encourage them about their feelings listener\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"I was really nervous to move across country.\", \"why were you?\",model_components, 'listener')\n",
    "\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Knew noone where we were moving and also far away from my mother_comma_ who is getting old.\", \"no one knew that you were moving?\",model_components, 'listener')\n",
    "\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"oh sorry_comma_ we knew no one where we were moving to\", \"oh_comma_ that has to be scary\",model_components, 'listener')\n",
    "\n",
    "exchange_processer.judge_exchange(pbc,flag_array,att_lst,\"Amazingly lol. But here I am.\", \"here you are, killing it\",model_components, 'listener')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c583c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'I hate when my wife and son are away from me', \"I get that you're feeling bad but do not let it get to you. I'm sure you'll be extra happy once they are here\",model_components)\n",
    "recommendation = exchange_processer.get_recommentation(pbc, processed_exchange,'listener')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56640823-d99a-4bf8-8ec4-9e016b6c95e2",
   "metadata": {},
   "source": [
    "### testing new VA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a480f61-da2e-4813-b79a-2517ab7ec77e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 3, 2, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.45\n",
      "0.4\n",
      "5\n",
      "7\n",
      "12\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m exchanges_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mprocessed_exchange\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m string_arr:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#print(ex)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     single_exchange, y_pred  \u001b[38;5;241m=\u001b[39m exchange_processer\u001b[38;5;241m.\u001b[39mpredict_exchange_empathy(pbc, flag_array, \u001b[38;5;241m1\u001b[39m, att_lst, ex[\u001b[38;5;241m0\u001b[39m], ex[\u001b[38;5;241m1\u001b[39m], model_components)\n\u001b[1;32m     51\u001b[0m     single_exchange[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred\n\u001b[1;32m     52\u001b[0m     exchanges_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([exchanges_df, single_exchange])\n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/exchange_processing.py:276\u001b[0m, in \u001b[0;36mpredict_exchange_empathy\u001b[0;34m(classifier, flag_array, ex_num, att_lst, speaker_utterance, listener_utterance, model_components)\u001b[0m\n\u001b[1;32m    274\u001b[0m exchange_df \u001b[38;5;241m=\u001b[39m exchange_df[att_lst] \u001b[38;5;66;03m#Feature order must match! \u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m#Predict empathy\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m empathy_prediction \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(exchange_df) \n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m#output exchange dataframe and prediction\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exchange_df, empathy_prediction[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/PBC4cip/core/PBC4cip.py:138\u001b[0m, in \u001b[0;36mPBC4cip.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 138\u001b[0m     scored_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_samples(X)  \n\u001b[1;32m    139\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m [ArgMax(instance) \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m scored_samples]\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/PBC4cip/core/PBC4cip.py:148\u001b[0m, in \u001b[0;36mPBC4cip.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    146\u001b[0m classification_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m tqdm(X, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying instances\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 148\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__predict_inst(instance)\n\u001b[1;32m    149\u001b[0m     classification_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classification_results\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/PBC4cip/core/PBC4cip.py:120\u001b[0m, in \u001b[0;36mPBC4cip.__predict_inst\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m votes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_nominal_feature)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEmergingPatterns:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pattern\u001b[38;5;241m.\u001b[39mIsMatch(instance):\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(votes)):\n\u001b[1;32m    122\u001b[0m             votes[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39mSupports[i]\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/PBC4cip/core/EmergingPatterns.py:60\u001b[0m, in \u001b[0;36mEmergingPattern.IsMatch\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mIsMatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mItems:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item\u001b[38;5;241m.\u001b[39mIsMatch(instance):\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/PBC4cip/core/Item.py:156\u001b[0m, in \u001b[0;36mGreatherThanItem.IsMatch\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mIsMatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[1;32m    155\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGetValue(instance)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(value):\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGetValue(instance) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mValue\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'I hate when my wife and son are away from me', \"I get that you're feeling bad but do not let it get to you. I'm sure you'll be extra happy once they are here\",model_components)\n",
    "\n",
    "\n",
    "\n",
    "conversations = ['body', 'comic', 'faith', 'joy', 'lottery', 'manager', 'racoon', 'sister', 'morning', 'furiosa']\n",
    "\n",
    "#conversations = ['body', 'comic', 'joy', 'lottery', 'racoon', 'sister', 'morning', 'furiosa']\n",
    "\n",
    "\n",
    "for weights in range(1,11,1):\n",
    "    print(weights/10)\n",
    "\n",
    "    #conversations =  ['faith']\n",
    "    empathy_truth = []\n",
    "    predictions_text = []\n",
    "    predictions_video = []\n",
    "\n",
    "\n",
    "    for i in range(len(conversations)):\n",
    "        convo_idx = i\n",
    "        convos_to_test = pd.read_csv(current_dir + '/useful_database_subsets/video_exchanges/convos_to_test.csv')\n",
    "\n",
    "        #print(convos_to_test.columns)\n",
    "\n",
    "        df_convo = convos_to_test[convos_to_test['conversation'] == conversations[convo_idx]].reset_index()\n",
    "\n",
    "        #print(convos_to_test[convos_to_test['conversation'] == conversations[convo_idx]])\n",
    "\n",
    "\n",
    "        string_arr = [\n",
    "                    [df_convo.loc[0,'speaker_utterance'],df_convo.loc[0,'listener_utterance']],\n",
    "                    [df_convo.loc[1,'speaker_utterance'],df_convo.loc[1,'listener_utterance']]\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "        exchanges_df = pd.DataFrame(columns=processed_exchange.columns)\n",
    "\n",
    "\n",
    "        for ex in string_arr:\n",
    "            #print(ex)\n",
    "            single_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst, ex[0], ex[1])\n",
    "            single_exchange['pred_text'] = y_pred\n",
    "            exchanges_df = pd.concat([exchanges_df, single_exchange])\n",
    "\n",
    "        exchanges_df = exchanges_df.reset_index(drop= True)\n",
    "\n",
    "        conversation = []\n",
    "        video_av_values = pd.read_csv(current_dir + '/useful_database_subsets/video_exchanges/exchanges/' + 'exchanges_'+str(conversations[convo_idx])+'.csv')\n",
    "        ratio_v_t = weights/10\n",
    "        exchanges_df['valence_speaker'] = (1-ratio_v_t)*exchanges_df['valence_speaker'] + (ratio_v_t)*video_av_values['valence_right']\n",
    "        exchanges_df['arousal_speaker'] = (1-ratio_v_t)*exchanges_df['arousal_speaker'] + (ratio_v_t)*video_av_values['arousal_right'] \n",
    "        exchanges_df['valence_listener'] = (1-ratio_v_t)*exchanges_df['valence_listener'] + (ratio_v_t)*video_av_values['valence_left']\n",
    "        exchanges_df['arousal_listener'] = (1-ratio_v_t)*exchanges_df['arousal_listener'] + (ratio_v_t)*video_av_values['arousal_left'] \n",
    "\n",
    "\n",
    "        #print(exchanges_df[['valence_speaker', 'valence_listener', 'arousal_listener', 'arousal_listener', 'pred_text']])\n",
    "\n",
    "        #mimicry\n",
    "        exchanges_df['emotional_similarity'] = exchanges_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "        exchanges_df['mimicry'] = exchanges_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "        exchanges_df = exchanges_df.drop(columns = ['emotional_similarity'])\n",
    "\n",
    "\n",
    "        exchanges_df['pred_video'] = pbc.predict(exchanges_df)\n",
    "        exchanges_df['pred_video'] = exchanges_df['pred_video'] + 1\n",
    "        exchanges_df['new_empathy_reduced'] = df_convo['new_empathy_reduced']\n",
    "        exchanges_df['empathy'] = df_convo['empathy']\n",
    "        exchanges_df.to_csv(current_dir + '/useful_database_subsets/video_exchanges/'+'exchange_predictions_'+str(conversations[convo_idx])+'.csv')\n",
    "        exchanges_df\n",
    "\n",
    "        empathy_truth.append(exchanges_df.loc[0,'new_empathy_reduced'])\n",
    "        empathy_truth.append(exchanges_df.loc[1,'new_empathy_reduced'])\n",
    "        predictions_text.append(exchanges_df.loc[0,'pred_text'])\n",
    "        predictions_text.append(exchanges_df.loc[1,'pred_text'])\n",
    "        predictions_video.append(exchanges_df.loc[0,'pred_video'])\n",
    "        predictions_video.append(exchanges_df.loc[1,'pred_video'])\n",
    "\n",
    "    for i in range(len(predictions_text)):\n",
    "        predictions_text[i] = predictions_text[i].astype(np.int64)\n",
    "\n",
    "    print(empathy_truth)\n",
    "    print(predictions_text)\n",
    "    print(predictions_video)\n",
    "\n",
    "    acc_text = accuracy_score(empathy_truth, predictions_text)\n",
    "    acc_video = accuracy_score(empathy_truth, predictions_video)\n",
    "\n",
    "    print(acc_text)\n",
    "    print(acc_video)\n",
    "\n",
    "    for i in range(len(empathy_truth)):\n",
    "        if empathy_truth[i] != predictions_video[i] and empathy_truth[i] == predictions_text[i]:\n",
    "            print(i+1)    \n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "632b22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n",
      "0.35\n",
      "5\n",
      "7\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_text = accuracy_score(empathy_truth, predictions_text)\n",
    "acc_video = accuracy_score(empathy_truth, predictions_video)\n",
    "\n",
    "print(acc_text)\n",
    "print(acc_video)\n",
    "\n",
    "for i in range(len(empathy_truth)):\n",
    "    if empathy_truth[i] != predictions_video[i] and empathy_truth[i] == predictions_text[i]:\n",
    "        print(i+1)\n",
    "\n",
    "conversations = ['body', 'comic', 'faith', 'joy', 'lottery', 'manager', 'racoon', 'sister', 'morning', 'furiosa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fe9c853c-df01-41bd-97ed-0959c5d34e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "   s_negative  s_neutral  s_positive  l_negative  l_neutral  l_positive  \\\n",
      "0    0.574606   0.387071    0.038323    0.375899   0.594197    0.029904   \n",
      "\n",
      "   predictions_ER  valence_speaker  arousal_speaker  valence_listener  ...  \\\n",
      "0               0         0.301333        -0.084667             0.021  ...   \n",
      "\n",
      "   agreeing  acknowledging  encouraging  consoling  sympathizing  suggesting  \\\n",
      "0  0.000062       0.000078     0.000021   0.000021      0.000109    0.000099   \n",
      "\n",
      "   questioning   wishing   neutral  mimicry  \n",
      "0     0.999338  0.000085  0.000187        1  \n",
      "\n",
      "[1 rows x 23 columns]\n",
      "   s_negative  s_neutral  s_positive  l_negative  l_neutral  l_positive  \\\n",
      "0    0.574606   0.387071    0.038323    0.375899   0.594197    0.029904   \n",
      "\n",
      "   predictions_ER  valence_speaker  arousal_speaker  valence_listener  ...  \\\n",
      "0               0        -0.061516         0.038489         -0.088388  ...   \n",
      "\n",
      "   agreeing  acknowledging  encouraging  consoling  sympathizing  suggesting  \\\n",
      "0  0.000062       0.000078     0.000021   0.000021      0.000109    0.000099   \n",
      "\n",
      "   questioning   wishing   neutral  mimicry  \n",
      "0     0.999338  0.000085  0.000187        1  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"Knew no one where we were moving_comma_ and also far away from my mother_comma_ who is getting old.\", \"no one knew that you were moving?\",model_components)\n",
    "print(y_pred)\n",
    "print(processed_exchange)\n",
    "processed_exchange.loc[0,'valence_speaker'] = -0.0615155920987168\n",
    "processed_exchange.loc[0,'arousal_speaker'] = 0.0384891806771198\n",
    "processed_exchange.loc[0,'valence_listener'] = -0.0883875858710437\n",
    "processed_exchange.loc[0,'arousal_listener'] = -0.0113459913457026\n",
    "processed_exchange['emotional_similarity'] = processed_exchange.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "processed_exchange['mimicry'] = processed_exchange.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "processed_exchange = processed_exchange.drop(columns = ['emotional_similarity'])\n",
    "print(processed_exchange)\n",
    "#print(processed_exchange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6798a2f2-791a-455d-a4a2-f10a340b01e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_exchange_empathy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_exchange, y_pred  \u001b[38;5;241m=\u001b[39m predict_exchange_empathy(pbc, flag_array, \u001b[38;5;241m1\u001b[39m, att_lst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHow are you?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoing pretty good, how about yourself?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_exchange)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_exchange_empathy' is not defined"
     ]
    }
   ],
   "source": [
    "processed_exchange, y_pred  = predict_exchange_empathy(pbc, flag_array, 1, att_lst, 'How are you?', \"Doing pretty good, how about yourself?\")\n",
    "print(y_pred)\n",
    "print(processed_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551b40e-cb48-437f-aba0-728c2b612575",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = [pbc, flag_array, 1, att_lst, 'How are you?', \"Doing pretty good, how about yourself?\"]\n",
    "arguments.extend(model_components)\n",
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy_source(*arguments)\n",
    "print(y_pred)\n",
    "print(processed_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdf55e-6c7e-4b12-abda-cc45dc23ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    bad_input_flag = True\n",
    "    while bad_input_flag:\n",
    "        utterance = input(\"Provide input: \")\n",
    "        if utterance.lower() == '':\n",
    "            print('Please provide valid input')\n",
    "        else:\n",
    "            bad_input_flag = False    \n",
    "    return utterance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60133a3e-af0a-45e3-a95b-8c15d1ae8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_exchange(pbc,flag_array,att_lst,'I hate when my wife and son are away from me', \"Oh ok. \",\"listener\")\n",
    "judge_exchange(pbc,flag_array,att_lst,'I hate when my wife and son are away from me', \"I've never been in that situation but that is understandable. \",\"listener\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c4847-da01-4ea1-91df-08ca06ba8d43",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5088618-fb00-4c0b-b2f0-4d091ce756c4",
   "metadata": {},
   "source": [
    "## Examples of inference and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde1d04-5fe6-4c4b-a439-ebca34d8c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "judge_exchange(pbc,flag_array,att_lst,'finally got my house, I do not have to deal with apartment living anymore', \"apartments are ok\",\"listener\")\n",
    "\n",
    "judge_exchange(pbc,flag_array,att_lst,'finally got my house, I do not have to deal with apartment living anymore', \"apartments are ok, you shouldn't knock them\",\"listener\")\n",
    "\n",
    "judge_exchange(pbc,flag_array,att_lst,'finally got my house, I do not have to deal with apartment living anymore', \"That's great I love living in my apartment but I'm happy for you\",\"listener\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffde58-acde-4dea-8d2c-3e73e9707f3d",
   "metadata": {},
   "source": [
    "## Automatic conversation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eb626-7772-4d41-9992-d50f1c170b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "spkr = [\"I cannot wait for the newest Pokemon game, it looks amazing to me!\", \"I see your point, but I still think it is fun\",'abortsequence']\n",
    "lstnr = ['pokemon is just ok',\"Oh well it's not bad. I had fun with pokemon when I was 10\", \"I relate to that, I listen to old songs from my childhood\",'abortsequence']\n",
    "\n",
    "speaker_utterances = []\n",
    "listener_utterances = []\n",
    "\n",
    "conversation_end = False\n",
    "#For a short conversation \n",
    "for i in range(2):    \n",
    "    if i>0:\n",
    "        j = i\n",
    "        while not good_speaker:\n",
    "            prompt = spkr[j]\n",
    "            print(f\"Speaker turn: {prompt}\")\n",
    "            print('Speaker_empathy')\n",
    "            good_speaker = judge_exchange(pbc, flag_array, att_lst, listener_utterances[i-1],prompt,'speaker')\n",
    "            j = j+1\n",
    "            #judgement_on_speaker = False\n",
    "            #print(f'judgement on speaker {judgement_on_speaker}')\n",
    "    else:      \n",
    "        prompt = spkr[i]\n",
    "        print(f\"Speaker turn: {prompt}\")\n",
    "        if 'abortsequence' in prompt:\n",
    "            break\n",
    "        good_speaker = False    \n",
    "        \n",
    "    \n",
    "    \n",
    "    speaker_utterances.append(prompt) #We append the successful utterance to the record\n",
    "    good_listener = False\n",
    "    #Keep listener hostage while they do not provide empathetic responses\n",
    "    j = i\n",
    "    while not good_listener:\n",
    "        answer = lstnr[j]\n",
    "        print(f\"Listener turn: {answer}\")\n",
    "        if 'abortsequence' in answer:\n",
    "            break\n",
    "        good_listener = judge_exchange(pbc, flag_array, att_lst, speaker_utterances[i], answer,'listener')\n",
    "        j = j+1\n",
    "        #print(f\"Speaker: {speaker_utterances[i]}\")\n",
    "        #print(f\"Listener: {listener_utterances[i]}\")     \n",
    "    listener_utterances.append(answer)\n",
    "\n",
    "\n",
    "print(speaker_utterances)\n",
    "print(listener_utterances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3c7ca-add5-4495-a3ea-3d82d9ad0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "spkr = [\"I cannot wait for the newest Pokemon game, it looks amazing to me!\", \"I see your point, but I still think it is fun\",'abortsequence']\n",
    "lstnr = ['pokemon is just ok',\"Oh well it's not bad. I had fun with pokemon when I was 10\", \"Yeah it was to me, maybe I should try the new one\",'abortsequence']\n",
    "\n",
    "speaker_utterances = []\n",
    "listener_utterances = []\n",
    "\n",
    "conversation_end = False\n",
    "#For a short conversation\n",
    "\n",
    "j = 0\n",
    "i = 0\n",
    "\n",
    "while not conversation_end:    \n",
    "    if j>0:\n",
    "        good_speaker = False    \n",
    "        while not good_speaker:\n",
    "            prompt = spkr[j]\n",
    "            print(f\"Speaker turn: {prompt}\")\n",
    "            print('Speaker_empathy')\n",
    "            good_speaker = judge_exchange(pbc, flag_array, att_lst, lstnr[i-1],prompt,'speaker')\n",
    "            j = j+1\n",
    "            if j + 1 >= len(spkr):\n",
    "                conversation_end = True\n",
    "            #judgement_on_speaker = False\n",
    "            #print(f'judgement on speaker {judgement_on_speaker}')\n",
    "    else:      \n",
    "        prompt = spkr[i]\n",
    "        print(f\"Speaker turn: {prompt}\")\n",
    "        if 'abortsequence' in prompt:\n",
    "            break\n",
    "        good_speaker = False    \n",
    "        if j + 1 >= len(spkr):\n",
    "            conversation_end = True\n",
    "        else:\n",
    "            j += 1        \n",
    "    speaker_utterances.append(prompt) #We append the successful utterance to the record\n",
    "    good_listener = False\n",
    "    #Keep listener hostage while they do not provide empathetic responses\n",
    "    while not good_listener:\n",
    "        answer = lstnr[i]\n",
    "        print(f\"Listener turn: {answer}\")\n",
    "        if 'abortsequence' in answer:\n",
    "            conversation_end = True\n",
    "            break\n",
    "        good_listener = judge_exchange(pbc, flag_array, att_lst, spkr[j], answer,'listener')\n",
    "        i = i+1\n",
    "        #print(f\"Speaker: {speaker_utterances[i]}\")\n",
    "        #print(f\"Listener: {listener_utterances[i]}\")     \n",
    "    listener_utterances.append(answer)\n",
    "    if i + 1 >= len(lstnr):\n",
    "        conversation_end = True\n",
    "    else:\n",
    "        i += 1\n",
    "    print(f'{i} {j}')\n",
    "        \n",
    "\n",
    "\n",
    "print(speaker_utterances)\n",
    "print(listener_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f6c87-f9cc-48d4-be62-7446f745bb74",
   "metadata": {},
   "source": [
    "## Conversation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbab16d-7ffc-4c51-92d6-1fc4adcedae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "for i in database['prompt']:\n",
    "    if(len(database[database['prompt'] == i]) >= 4 ):\n",
    "        #print(database[database['prompt'] == i])\n",
    "        candidates.append(i)\n",
    "set(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a73bf1-fff0-40cc-97ca-839dfc85513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"First exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"i loved taking care of my sisters pet\", \"Huh, is that so\", 'listener')\n",
    "judge_exchange(pbc,flag_array,att_lst,\"i loved taking care of my sisters pet\", \"It's cool that you loved that\", 'listener')\n",
    "print(\"Second exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"Yeah! I have loved animals since then especially dogs\", \"Dogs are very cute. Cats too\", \"listener\")\n",
    "print(\"Third exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"Oh my gosh yes. Cats are so fluffy and cuddly\", \"They are! I love petting them\", \"listener\")\n",
    "print(\"Fourth exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"You know i really enjoy having pets they bring a new life into an empty feeling house\", \"Yes I only have one cat. How about you?\", \"listener\")\n",
    "print(\"Fifth exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"we have a cat, a dog, a bunny, and a betta fish!\", \"Those are many pets\", \"listener\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"we have a cat, a dog, a bunny, and a betta fish!\", \"Those are many pets, how do you manage?\", \"listener\")\n",
    "print(\"Six exchange\")\n",
    "judge_exchange(pbc,flag_array,att_lst,\"It is a lot of work. But their little faces are so worth it\", \"Yes they are I bet you feel proud\", \"listener\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedae12-ac58-4649-8df3-c2161950c2c9",
   "metadata": {},
   "source": [
    "# Miscellaneous zone\n",
    "\n",
    "Where we do all sorts of experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c868fd",
   "metadata": {},
   "source": [
    "### Checking pattern list that covers an exchange (values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7856afa1-792f-4613-b316-1079451b71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s_negative': [0.9398489], 's_neutral': [0.057078857], 's_positive': [0.0030721908], 'l_negative': [0.33885983], 'l_neutral': [0.64117646], 'l_positive': [0.019963712], 'predictions_ER': [0], 'valence_speaker': [0], 'arousal_speaker': [2], 'dominance_speaker': [-0.708], 'valence_listener': [0.656], 'arousal_listener': [-0.63], 'dominance_listener': [-0.98], 's_word_len': [0.55], 'l_word_len': [-0.51], 'agreeing': [7], 'acknowledging': [5], 'encouraging': [6.73560498398729e-05], 'consoling': [5.48729512956925e-05], 'sympathizing': [3.37222991220187e-05], 'suggesting': [1.91891886061057e-05], 'questioning': [0.000120048694953], 'wishing': [0.0005292572896], 'neutral': [0.998958706855774], 'mimicry': [0.000112920177344]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emerging_patterns = pbc.EmergingPatterns\n",
    "emerging_patterns[0].Items[0].Value\n",
    "dummy_dic = {}\n",
    "att_lst = pbc.dataset.Attributes\n",
    "values = [0.9398489, 0.057078857, 0.0030721908, 0.33885983, 0.64117646, \n",
    "    0.019963712, 0, 0, 2, -0.708, 0.656, -0.63, -0.98, 0.55, -0.51, \n",
    "    7, 5, 6.73560498398729E-05, 5.48729512956925E-05, 3.37222991220187E-05, \n",
    "    1.91891886061057E-05, 0.000120048694953, 0.0005292572896, 0.998958706855774, \n",
    "    0.000112920177344, 0.000103849401057, 1]\n",
    "\n",
    "for i in range(len(att_lst)):\n",
    "    new_data = {str(att_lst[i][0]): [values[i]]}\n",
    "    dummy_dic.update(new_data)\n",
    "\n",
    "\n",
    "print(dummy_dic)\n",
    "\n",
    "exchange_df = pd.DataFrame.from_dict(dummy_dic)\n",
    "\n",
    "pbc.predict(exchange_df)\n",
    "\n",
    "\n",
    "pattern_list = [] #patterns that cover the exchange\n",
    "\n",
    "for instance in exchange_df.to_numpy(): \n",
    "    for pattern in emerging_patterns:\n",
    "        if pattern.IsMatch(instance):\n",
    "            pattern_list.append(pattern)   \n",
    "len(pattern_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef5bba9-50e6-477a-ae5d-2d87e23ed6ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_exchange_empathy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_exchange, y_pred  \u001b[38;5;241m=\u001b[39m predict_exchange_empathy(pbc, flag_array, \u001b[38;5;241m1\u001b[39m, att_lst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt the time there was a friend that told me that i could not jump over him_comma_ then i jumped over him.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeato burrito\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_exchange_empathy' is not defined"
     ]
    }
   ],
   "source": [
    "processed_exchange, y_pred  = predict_exchange_empathy(pbc, flag_array, 1, att_lst, 'At the time there was a friend that told me that i could not jump over him_comma_ then i jumped over him.', \"Neato burrito\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a269fd-f3c1-4893-9c0e-a0911fe6c49e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'judge_exchange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m judge_exchange(pbc,flag_array,att_lst,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt the time there was a friend that told me that i could not jump over him_comma_ then i jumped over him.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeato burrito\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistener\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'judge_exchange' is not defined"
     ]
    }
   ],
   "source": [
    "judge_exchange(pbc,flag_array,att_lst,'At the time there was a friend that told me that i could not jump over him_comma_ then i jumped over him.', \"Neato burrito\", 'listener')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ef456-f3cf-4a11-af11-12c000b54f2c",
   "metadata": {},
   "source": [
    "### load predictions on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9817586a-500a-4d94-8b06-4a1c778ab3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_utterance</th>\n",
       "      <th>listener_utterance</th>\n",
       "      <th>s_negative</th>\n",
       "      <th>s_neutral</th>\n",
       "      <th>s_positive</th>\n",
       "      <th>l_negative</th>\n",
       "      <th>l_neutral</th>\n",
       "      <th>...</th>\n",
       "      <th>encouraging</th>\n",
       "      <th>consoling</th>\n",
       "      <th>sympathizing</th>\n",
       "      <th>suggesting</th>\n",
       "      <th>questioning</th>\n",
       "      <th>wishing</th>\n",
       "      <th>neutral</th>\n",
       "      <th>mimicry</th>\n",
       "      <th>empathy</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:10889_conv:21779</td>\n",
       "      <td>jealous</td>\n",
       "      <td>I saw my neighbor bought the car I have always...</td>\n",
       "      <td>I know_comma_ it would look perfect in front o...</td>\n",
       "      <td>Time goes by so fast. You will see.</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.827828</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>0.536051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:1916_conv:3833</td>\n",
       "      <td>hopeful</td>\n",
       "      <td>when you expect more you have been disapponted...</td>\n",
       "      <td>yeah good. whats your name?</td>\n",
       "      <td>I am not sure I feel comfortable telling you a...</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.183143</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.453844</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.145248</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.725658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:7038_conv:14076</td>\n",
       "      <td>impressed</td>\n",
       "      <td>My coworker did this presentation at work that...</td>\n",
       "      <td>I was really proud of my coworker and their pr...</td>\n",
       "      <td>That is nice. It is good to be supportive</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>0.991373</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:10237_conv:20475</td>\n",
       "      <td>devastated</td>\n",
       "      <td>One time my dog got run over by a car. He had ...</td>\n",
       "      <td>Yeah. He did not survive. I really miss him</td>\n",
       "      <td>Hope everything gets better soon</td>\n",
       "      <td>0.921586</td>\n",
       "      <td>0.069954</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.086787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:5988_conv:11976</td>\n",
       "      <td>prepared</td>\n",
       "      <td>I have been working all week on my project. To...</td>\n",
       "      <td>Hi_comma_ I have a big business presentation t...</td>\n",
       "      <td>Are you fully prepared for it?</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.235530</td>\n",
       "      <td>0.762036</td>\n",
       "      <td>0.139080</td>\n",
       "      <td>0.810800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.998911</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>hit:9741_conv:19482</td>\n",
       "      <td>confident</td>\n",
       "      <td>I had to give a speech in front of a few of my...</td>\n",
       "      <td>I absolutely hate public speaking. I did alrig...</td>\n",
       "      <td>I bet! Well it over now and I am sure you did ...</td>\n",
       "      <td>0.949689</td>\n",
       "      <td>0.044111</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.031774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.990227</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>hit:3981_conv:7962</td>\n",
       "      <td>jealous</td>\n",
       "      <td>My friend bought a new car.</td>\n",
       "      <td>An old beat up car that I need to replace but ...</td>\n",
       "      <td>I drive a gianormous conversion van</td>\n",
       "      <td>0.949371</td>\n",
       "      <td>0.045157</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.222195</td>\n",
       "      <td>0.711267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>hit:2853_conv:5707</td>\n",
       "      <td>anxious</td>\n",
       "      <td>I was nervous when I had to go to jury duty. I...</td>\n",
       "      <td>No_comma_ I had no idea what to expect</td>\n",
       "      <td>I have never served on jury duty either. The c...</td>\n",
       "      <td>0.560171</td>\n",
       "      <td>0.411278</td>\n",
       "      <td>0.028551</td>\n",
       "      <td>0.427809</td>\n",
       "      <td>0.534693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.860535</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>hit:8397_conv:16795</td>\n",
       "      <td>terrified</td>\n",
       "      <td>Running up the stairs in the dark when I was a...</td>\n",
       "      <td>Yeah_comma_ so I would book it up the stairs a...</td>\n",
       "      <td>I am sure I would too. Just be careful</td>\n",
       "      <td>0.027067</td>\n",
       "      <td>0.625647</td>\n",
       "      <td>0.347286</td>\n",
       "      <td>0.148689</td>\n",
       "      <td>0.703990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>hit:4484_conv:8169</td>\n",
       "      <td>caring</td>\n",
       "      <td>My friend has had the flu_comma_ and I have be...</td>\n",
       "      <td>My friend has had the flu_comma_ and I have be...</td>\n",
       "      <td>That is sweet of you_comma_ the flu is a prett...</td>\n",
       "      <td>0.088572</td>\n",
       "      <td>0.542803</td>\n",
       "      <td>0.368626</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.064247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3303 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   conv_id     context  \\\n",
       "0     hit:10889_conv:21779     jealous   \n",
       "1       hit:1916_conv:3833     hopeful   \n",
       "2      hit:7038_conv:14076   impressed   \n",
       "3     hit:10237_conv:20475  devastated   \n",
       "4      hit:5988_conv:11976    prepared   \n",
       "...                    ...         ...   \n",
       "3298   hit:9741_conv:19482   confident   \n",
       "3299    hit:3981_conv:7962     jealous   \n",
       "3300    hit:2853_conv:5707     anxious   \n",
       "3301   hit:8397_conv:16795   terrified   \n",
       "3302    hit:4484_conv:8169      caring   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     I saw my neighbor bought the car I have always...   \n",
       "1     when you expect more you have been disapponted...   \n",
       "2     My coworker did this presentation at work that...   \n",
       "3     One time my dog got run over by a car. He had ...   \n",
       "4     I have been working all week on my project. To...   \n",
       "...                                                 ...   \n",
       "3298  I had to give a speech in front of a few of my...   \n",
       "3299                        My friend bought a new car.   \n",
       "3300  I was nervous when I had to go to jury duty. I...   \n",
       "3301  Running up the stairs in the dark when I was a...   \n",
       "3302  My friend has had the flu_comma_ and I have be...   \n",
       "\n",
       "                                      speaker_utterance  \\\n",
       "0     I know_comma_ it would look perfect in front o...   \n",
       "1                           yeah good. whats your name?   \n",
       "2     I was really proud of my coworker and their pr...   \n",
       "3           Yeah. He did not survive. I really miss him   \n",
       "4     Hi_comma_ I have a big business presentation t...   \n",
       "...                                                 ...   \n",
       "3298  I absolutely hate public speaking. I did alrig...   \n",
       "3299  An old beat up car that I need to replace but ...   \n",
       "3300             No_comma_ I had no idea what to expect   \n",
       "3301  Yeah_comma_ so I would book it up the stairs a...   \n",
       "3302  My friend has had the flu_comma_ and I have be...   \n",
       "\n",
       "                                     listener_utterance  s_negative  \\\n",
       "0                   Time goes by so fast. You will see.    0.026005   \n",
       "1     I am not sure I feel comfortable telling you a...    0.005329   \n",
       "2             That is nice. It is good to be supportive    0.001078   \n",
       "3                      Hope everything gets better soon    0.921586   \n",
       "4                        Are you fully prepared for it?    0.002434   \n",
       "...                                                 ...         ...   \n",
       "3298  I bet! Well it over now and I am sure you did ...    0.949689   \n",
       "3299                I drive a gianormous conversion van    0.949371   \n",
       "3300  I have never served on jury duty either. The c...    0.560171   \n",
       "3301             I am sure I would too. Just be careful    0.027067   \n",
       "3302  That is sweet of you_comma_ the flu is a prett...    0.088572   \n",
       "\n",
       "      s_neutral  s_positive  l_negative  l_neutral  ...  encouraging  \\\n",
       "0      0.146168    0.827828    0.273078   0.536051  ...     0.000544   \n",
       "1      0.183143    0.811529    0.453844   0.520900  ...     0.000467   \n",
       "2      0.007549    0.991373    0.002196   0.014970  ...     0.000377   \n",
       "3      0.069954    0.008460    0.004004   0.086787  ...     0.002373   \n",
       "4      0.235530    0.762036    0.139080   0.810800  ...     0.000093   \n",
       "...         ...         ...         ...        ...  ...          ...   \n",
       "3298   0.044111    0.006201    0.002755   0.031774  ...     0.000148   \n",
       "3299   0.045157    0.005472    0.222195   0.711267  ...     0.000318   \n",
       "3300   0.411278    0.028551    0.427809   0.534693  ...     0.000381   \n",
       "3301   0.625647    0.347286    0.148689   0.703990  ...     0.000161   \n",
       "3302   0.542803    0.368626    0.021817   0.064247  ...     0.000052   \n",
       "\n",
       "      consoling  sympathizing  suggesting  questioning   wishing   neutral  \\\n",
       "0      0.000172      0.000325    0.000067     0.000040  0.007855  0.000388   \n",
       "1      0.145248      0.003519    0.002405     0.002545  0.000153  0.725658   \n",
       "2      0.000236      0.000155    0.000037     0.000073  0.003338  0.000069   \n",
       "3      0.995767      0.000493    0.000107     0.000336  0.000064  0.000093   \n",
       "4      0.000012      0.000063    0.000195     0.998911  0.000307  0.000167   \n",
       "...         ...           ...         ...          ...       ...       ...   \n",
       "3298   0.000034      0.000142    0.000117     0.000241  0.000037  0.990227   \n",
       "3299   0.002807      0.000196    0.000047     0.000063  0.001068  0.001629   \n",
       "3300   0.001946      0.000405    0.000127     0.000545  0.000121  0.860535   \n",
       "3301   0.000254      0.000065    0.000031     0.000078  0.001583  0.001291   \n",
       "3302   0.000034      0.000139    0.000092     0.000104  0.000596  0.000366   \n",
       "\n",
       "      mimicry  empathy  pred  \n",
       "0           1        3     1  \n",
       "1           1        1     2  \n",
       "2           1        3     3  \n",
       "3           0        1     2  \n",
       "4           1        3     1  \n",
       "...       ...      ...   ...  \n",
       "3298        0        3     3  \n",
       "3299        1        1     1  \n",
       "3300        0        1     3  \n",
       "3301        1        2     2  \n",
       "3302        1        2     3  \n",
       "\n",
       "[3303 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pred_pbc = pd.read_csv(current_dir + '/best_predictions.txt', header = None)\n",
    "pred_pbc = pred_pbc.rename(columns = {0:'pred'})\n",
    "pred_pbc['pred'] = pred_pbc['pred'].apply(lambda x: x + 1)\n",
    "pred_pbc\n",
    "\n",
    "database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv'\n",
    "\n",
    "database = pd.read_csv(current_dir + database_dir)\n",
    "X = database.drop(columns=['empathy'])\n",
    "y = database['empathy']\n",
    "\n",
    "test_db = pd.read_csv(current_dir + test_database_dir)\n",
    "test_db\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df = test_df.reset_index(drop = True)\n",
    "test_df['pred'] = pred_pbc['pred']\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf315cb-22a6-48c0-b723-3171e5813227",
   "metadata": {},
   "source": [
    "## measuring performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0834c13-10ac-41e9-aeda-f05b9b9de73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "3298    1\n",
      "3299    0\n",
      "3300    1\n",
      "3301    1\n",
      "3302    1\n",
      "Name: pred, Length: 3303, dtype: int64\n",
      "      empathy\n",
      "0           2\n",
      "1           1\n",
      "2           2\n",
      "3           1\n",
      "4           2\n",
      "...       ...\n",
      "3298        2\n",
      "3299        1\n",
      "3300        1\n",
      "3301        2\n",
      "3302        2\n",
      "\n",
      "[3303 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7025794952465386"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import CEM as cem\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features = test_df.columns\n",
    "features2keep = ['conv_id', 'context', 'prompt', 'speaker_utterance','listener_utterance','valence_listener','arousal_listener','empathy']\n",
    "\n",
    "features2delete = list(set(features) - set(features2keep))\n",
    "\n",
    "test_df = test_df.drop(columns = features2delete)\n",
    "\n",
    "\n",
    "test_df['pred'] = pred_pbc['pred']\n",
    "\n",
    "#x_test = test_df.drop(columns=['empathy'])\n",
    "#y_test = test_df.copy()\n",
    "#y_test = y_test.drop(columns=x_test.columns)\n",
    "\n",
    "\n",
    "#ClosenessEvaluationMeasure = cem.get_cem(test_df['pred'].apply(lambda x: x - 1),y_test)\n",
    "\n",
    "\n",
    "\n",
    "test_df['empathy_red'] = test_df.apply(lambda x: 2 if (x['empathy'] == 3 or x['empathy'] == 2)  else 1, axis = 1)\n",
    "test_df['pred_red'] = test_df.apply(lambda x: 1 if (x['pred'] == 3 or x['pred'] == 2)  else 0, axis = 1)\n",
    "test_df_2 = test_df.drop(columns=['empathy'])\n",
    "test_df_2 = test_df_2.drop(columns=['pred'])\n",
    "\n",
    "\n",
    "test_df_2 = test_df_2.rename(columns={\"empathy_red\": \"empathy\"})\n",
    "test_df_2 = test_df_2.rename(columns={\"pred_red\": \"pred\"})\n",
    "feature_columns = test_df_2.drop(columns=['empathy']).columns\n",
    "#print(test_df_2.head())\n",
    "#correct_pred_only = test_df[test_df['empathy'] ==  test_df['pred']]\n",
    "#correct_pred_only_low = correct_pred_only[correct_pred_only['empathy'] ==  1]\n",
    "\n",
    "#no_valence = test_df[test_df['valence_listener'] == 0]\n",
    "\n",
    "#no_valence = test_df[test_df['valence_listener'] > -0.1]\n",
    "#no_valence = no_valence[no_valence['valence_listener'] < 0.1]\n",
    "#no_valence = no_valence[no_valence['empathy'] != no_valence['pred']]\n",
    "\n",
    "\n",
    "#no_valence.to_csv('no_valence_wrong_label_examples.csv')\n",
    "#correct_pred_only.to_csv('correctly_predicted_exchanges.csv')\n",
    "#correct_pred_only_low.to_csv('correctly_predicted_exchanges_low.csv')\n",
    "\n",
    "#correct_pred_only_low['context'].describe()\n",
    "\n",
    "acc = accuracy_score(test_df['empathy_red'], test_df['pred_red'])\n",
    "y_pred = test_df_2['pred']\n",
    "print(y_pred)\n",
    "y_true = test_df_2.drop(columns = feature_columns)\n",
    "print(y_true)\n",
    "ClosenessEvaluationMeasure = cem.get_cem(y_pred,y_true)\n",
    "test_df\n",
    "\n",
    "test_df_2\n",
    "ClosenessEvaluationMeasure\n",
    "ClosenessEvaluationMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c7559-b3cb-4e70-86e2-f6a15e565813",
   "metadata": {},
   "source": [
    "### get full conversations on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c785fc-a6ad-40e3-8728-1ae08c45668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2939\n",
      "3303\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pred_pbc = pd.read_csv(current_dir + '/best_predictions.txt', header = None)\n",
    "pred_pbc = pred_pbc.rename(columns = {0:'pred'})\n",
    "pred_pbc['pred'] = pred_pbc['pred'].apply(lambda x: x + 1)\n",
    "pred_pbc\n",
    "\n",
    "database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv'\n",
    "\n",
    "database = pd.read_csv(current_dir + database_dir)\n",
    "X = database.drop(columns=['empathy'])\n",
    "y = database['empathy']\n",
    "\n",
    "test_db = pd.read_csv(current_dir + test_database_dir)\n",
    "test_db\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df = test_df.reset_index(drop = True)\n",
    "test_df['pred'] = pred_pbc['pred']\n",
    "print(len(test_df['conv_id'].unique()))\n",
    "print(len(test_df['conv_id']))\n",
    "\n",
    "\n",
    "ids = test_df[\"conv_id\"]\n",
    "duplicated_convos = test_df[ids.isin(ids[ids.duplicated()])].sort_values(\"conv_id\")\n",
    "\n",
    "duplicated_convos\n",
    "\n",
    "v = duplicated_convos.conv_id.value_counts()\n",
    "duplicated_convos[duplicated_convos.conv_id.isin(v.index[v.gt(2)])]\n",
    "duplicated_convos\n",
    "\n",
    "duplicated_convos = duplicated_convos[['conv_id', 'prompt', 'context', 'speaker_utterance', 'listener_utterance', 'empathy', 'pred']]\n",
    "\n",
    "\n",
    "duplicated_convos.to_csv('./useful_database_subsets/convos_in_test_set.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "duplicated_convos = em_red.reduce_emotion_labels_to_8('context',duplicated_convos)\n",
    "duplicated_convos.to_csv('./useful_database_subsets/convos_in_test_set_reduced_emotion.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(len(duplicated_convos['conv_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41676f-48cc-4476-9220-19b042882ef6",
   "metadata": {},
   "source": [
    "### get emotionally balanced contexts from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e65811-3422-4dfd-9159-846d20b08504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indexes of the conversations already sampled\n",
    "def get_index_list(approved_df,dataframe_list):\n",
    "  index_list = []\n",
    "  for convo in approved_df['prompt']:\n",
    "    #print(convo)\n",
    "    for  i in range(len(dataframe_list)):\n",
    "      if dataframe_list[i].loc[dataframe_list[i]['prompt'] == convo].empty == False:\n",
    "        index_list.append([i,dataframe_list[i].loc[dataframe_list[i]['prompt'] == convo].index])\n",
    "            #index_list.append([frame.index,frame.loc[frame['conv_id'] == convo].index])\n",
    "  return index_list\n",
    "\n",
    "\n",
    "#Function that deletes the conversations already sampled from the database\n",
    "def remove_accepted_convos(index_list, base_df):\n",
    "  for index in index_list:\n",
    "    base_df = base_df.drop([index[1][0]])\n",
    "  return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5610d425-f7b8-45e0-9f3f-4bd010524344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7996\n",
      "16512\n",
      "7899\n"
     ]
    }
   ],
   "source": [
    "database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv'\n",
    "\n",
    "database = pd.read_csv(current_dir + database_dir)\n",
    "database\n",
    "#starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "\n",
    "\n",
    "print(len(database['conv_id'].unique()))\n",
    "print(len(database))\n",
    "\n",
    "\n",
    "print(len(database['prompt'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94c9353-a10a-49f4-ad61-69429142ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>empathy</th>\n",
       "      <th>speaker_utterance</th>\n",
       "      <th>listener_utterance</th>\n",
       "      <th>s_negative</th>\n",
       "      <th>s_neutral</th>\n",
       "      <th>s_positive</th>\n",
       "      <th>l_negative</th>\n",
       "      <th>...</th>\n",
       "      <th>agreeing</th>\n",
       "      <th>acknowledging</th>\n",
       "      <th>encouraging</th>\n",
       "      <th>consoling</th>\n",
       "      <th>sympathizing</th>\n",
       "      <th>suggesting</th>\n",
       "      <th>questioning</th>\n",
       "      <th>wishing</th>\n",
       "      <th>neutral</th>\n",
       "      <th>mimicry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:9071_conv:18143</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I was discussing phone bills with my friend_co...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I was discussing phone bills with my friend.  ...</td>\n",
       "      <td>That must be nice</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.757237</td>\n",
       "      <td>0.071505</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.997114</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:9071_conv:18143</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I was discussing phone bills with my friend_co...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yeah I know.  Although I like to think I would...</td>\n",
       "      <td>Yeah_comma_ I think we all wish that was possible</td>\n",
       "      <td>0.545449</td>\n",
       "      <td>0.403581</td>\n",
       "      <td>0.050971</td>\n",
       "      <td>0.033245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.975375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:1914_conv:3829</td>\n",
       "      <td>disgust</td>\n",
       "      <td>My daughter was sick the other day and I could...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My daughter was sick the other day and I could...</td>\n",
       "      <td>That is an unfortunate predicament to be in. W...</td>\n",
       "      <td>0.933043</td>\n",
       "      <td>0.060932</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.885590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.997194</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:1914_conv:3829</td>\n",
       "      <td>disgust</td>\n",
       "      <td>My daughter was sick the other day and I could...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I sent her to daycare even though I probably s...</td>\n",
       "      <td>That is a tough decision for sure_comma_ but y...</td>\n",
       "      <td>0.548584</td>\n",
       "      <td>0.409580</td>\n",
       "      <td>0.041835</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.997575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:7464_conv:14928</td>\n",
       "      <td>joy</td>\n",
       "      <td>My son has not always been a strong swimmer.  ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I just have to brag on my son. He has not alwa...</td>\n",
       "      <td>That is amazing hard work pays off and I think...</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.017958</td>\n",
       "      <td>0.980578</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562864</td>\n",
       "      <td>0.410589</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.023885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>hit:2945_conv:5891</td>\n",
       "      <td>disgust</td>\n",
       "      <td>I say every year I'm going to wear a two peice...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>One day.</td>\n",
       "      <td>Just keep working at it.</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>0.603897</td>\n",
       "      <td>0.332442</td>\n",
       "      <td>0.029027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.996525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>hit:6349_conv:12698</td>\n",
       "      <td>sadness</td>\n",
       "      <td>a younger guy i used to work with passed away ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a guy i used to work with passed away this wee...</td>\n",
       "      <td>Oh Im so sorry to hear that</td>\n",
       "      <td>0.550541</td>\n",
       "      <td>0.412104</td>\n",
       "      <td>0.037356</td>\n",
       "      <td>0.848501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>hit:6349_conv:12698</td>\n",
       "      <td>sadness</td>\n",
       "      <td>a younger guy i used to work with passed away ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yeah i feel so bad for his family</td>\n",
       "      <td>Were you close?</td>\n",
       "      <td>0.951179</td>\n",
       "      <td>0.044386</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.100740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.997412</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>hit:4042_conv:8084</td>\n",
       "      <td>joy</td>\n",
       "      <td>The first time I met my long distance girlfrie...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I have been seeing someone on line for over 2 ...</td>\n",
       "      <td>That is awesome! How did it go?</td>\n",
       "      <td>0.045149</td>\n",
       "      <td>0.473167</td>\n",
       "      <td>0.481684</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.998701</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>hit:4042_conv:8084</td>\n",
       "      <td>joy</td>\n",
       "      <td>The first time I met my long distance girlfrie...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I live with her now. In fact she is sitting ri...</td>\n",
       "      <td>That is so cool. I am glad you found your soul...</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>0.441915</td>\n",
       "      <td>0.550711</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.995740</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows  33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   conv_id  context  \\\n",
       "0      hit:9071_conv:18143  sadness   \n",
       "1      hit:9071_conv:18143  sadness   \n",
       "2       hit:1914_conv:3829  disgust   \n",
       "3       hit:1914_conv:3829  disgust   \n",
       "4      hit:7464_conv:14928      joy   \n",
       "...                    ...      ...   \n",
       "16507   hit:2945_conv:5891  disgust   \n",
       "16508  hit:6349_conv:12698  sadness   \n",
       "16509  hit:6349_conv:12698  sadness   \n",
       "16510   hit:4042_conv:8084      joy   \n",
       "16511   hit:4042_conv:8084      joy   \n",
       "\n",
       "                                                  prompt  empathy  \\\n",
       "0      I was discussing phone bills with my friend_co...      4.0   \n",
       "1      I was discussing phone bills with my friend_co...      4.0   \n",
       "2      My daughter was sick the other day and I could...      5.0   \n",
       "3      My daughter was sick the other day and I could...      5.0   \n",
       "4      My son has not always been a strong swimmer.  ...      5.0   \n",
       "...                                                  ...      ...   \n",
       "16507  I say every year I'm going to wear a two peice...      4.0   \n",
       "16508  a younger guy i used to work with passed away ...      3.0   \n",
       "16509  a younger guy i used to work with passed away ...      3.0   \n",
       "16510  The first time I met my long distance girlfrie...      4.0   \n",
       "16511  The first time I met my long distance girlfrie...      4.0   \n",
       "\n",
       "                                       speaker_utterance  \\\n",
       "0      I was discussing phone bills with my friend.  ...   \n",
       "1      Yeah I know.  Although I like to think I would...   \n",
       "2      My daughter was sick the other day and I could...   \n",
       "3      I sent her to daycare even though I probably s...   \n",
       "4      I just have to brag on my son. He has not alwa...   \n",
       "...                                                  ...   \n",
       "16507                                           One day.   \n",
       "16508  a guy i used to work with passed away this wee...   \n",
       "16509                  yeah i feel so bad for his family   \n",
       "16510  I have been seeing someone on line for over 2 ...   \n",
       "16511  I live with her now. In fact she is sitting ri...   \n",
       "\n",
       "                                      listener_utterance  s_negative  \\\n",
       "0                                      That must be nice    0.171257   \n",
       "1      Yeah_comma_ I think we all wish that was possible    0.545449   \n",
       "2      That is an unfortunate predicament to be in. W...    0.933043   \n",
       "3      That is a tough decision for sure_comma_ but y...    0.548584   \n",
       "4      That is amazing hard work pays off and I think...    0.001463   \n",
       "...                                                  ...         ...   \n",
       "16507                          Just keep working at it.     0.063661   \n",
       "16508                        Oh Im so sorry to hear that    0.550541   \n",
       "16509                                    Were you close?    0.951179   \n",
       "16510                    That is awesome! How did it go?    0.045149   \n",
       "16511  That is so cool. I am glad you found your soul...    0.007374   \n",
       "\n",
       "       s_neutral  s_positive  l_negative  ...  agreeing  acknowledging  \\\n",
       "0       0.757237    0.071505    0.005727  ...  0.000341       0.997114   \n",
       "1       0.403581    0.050971    0.033245  ...  0.000300       0.000081   \n",
       "2       0.060932    0.006025    0.885590  ...  0.000067       0.000088   \n",
       "3       0.409580    0.041835    0.866907  ...  0.001575       0.000145   \n",
       "4       0.017958    0.980578    0.002007  ...  0.562864       0.410589   \n",
       "...          ...         ...         ...  ...       ...            ...   \n",
       "16507   0.603897    0.332442    0.029027  ...  0.000134       0.002473   \n",
       "16508   0.412104    0.037356    0.848501  ...  0.000156       0.002233   \n",
       "16509   0.044386    0.004436    0.100740  ...  0.000058       0.000068   \n",
       "16510   0.473167    0.481684    0.001490  ...  0.000065       0.000501   \n",
       "16511   0.441915    0.550711    0.001266  ...  0.000530       0.995740   \n",
       "\n",
       "       encouraging  consoling  sympathizing  suggesting  questioning  \\\n",
       "0         0.000295   0.000305      0.000102    0.000037     0.000071   \n",
       "1         0.000060   0.000068      0.000618    0.017769     0.003114   \n",
       "2         0.000019   0.000028      0.000526    0.001284     0.997194   \n",
       "3         0.000016   0.000323      0.000073    0.000069     0.000165   \n",
       "4         0.000710   0.000386      0.000183    0.000219     0.000381   \n",
       "...            ...        ...           ...         ...          ...   \n",
       "16507     0.000020   0.000043      0.000051    0.000137     0.000528   \n",
       "16508     0.000028   0.000041      0.996461    0.000085     0.000417   \n",
       "16509     0.000028   0.000019      0.000200    0.001557     0.997412   \n",
       "16510     0.000021   0.000013      0.000159    0.000157     0.998701   \n",
       "16511     0.000232   0.000105      0.000129    0.000046     0.000087   \n",
       "\n",
       "        wishing   neutral  mimicry  \n",
       "0      0.001667  0.000067        1  \n",
       "1      0.002616  0.975375        1  \n",
       "2      0.000106  0.000688        0  \n",
       "3      0.000060  0.997575        1  \n",
       "4      0.000785  0.023885        1  \n",
       "...         ...       ...      ...  \n",
       "16507  0.000090  0.996525        1  \n",
       "16508  0.000498  0.000080        1  \n",
       "16509  0.000184  0.000473        0  \n",
       "16510  0.000127  0.000255        0  \n",
       "16511  0.003045  0.000087        1  \n",
       "\n",
       "[16512 rows x 33 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_emo_prompt_df = em_red.reduce_emotion_labels_to_8('context',database)\n",
    "red_emo_prompt_df\n",
    "print(len(red_emo_prompt_df['context'].unique()))\n",
    "\n",
    "red_emo_prompt_df\n",
    "\n",
    "red_emo_prompt_df.to_csv('EmpatheticExchanges_prompts_reduced_emotion.csv',index=False)\n",
    "red_emo_prompt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af87598-0d5b-4b01-b0d4-e0f78dc4801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'joy']\n",
    "anger_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'anger']\n",
    "disgust_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'disgust']\n",
    "fear_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'fear']\n",
    "trust_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'trust']\n",
    "surprise_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'surprise']\n",
    "sadness_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'sadness']\n",
    "anticipation_df = red_emo_prompt_df[red_emo_prompt_df['context']== 'anticipation']\n",
    "\n",
    "emo_df_lst = [joy_df,anger_df,disgust_df,fear_df,trust_df,surprise_df,sadness_df,anticipation_df]\n",
    "\n",
    "def get_sampled_dataframe(df_lst, extra_emotion):\n",
    "    emo_to_num = {'joy': 0, 'anger': 1, 'disgust': 2, 'fear': 3,'trust': 4,'surprise': 5,'sadness': 6,'anticipation': 7}\n",
    "    extra_index = emo_to_num[extra_emotion]\n",
    "    accepted_flag = False\n",
    "    dataframe_samples = []\n",
    "    for df in df_lst:\n",
    "        dataframe_samples.append(df.sample(n=1))\n",
    "    dataframe_samples.append(df_lst[extra_index].sample(n=1))\n",
    "    #Join samples in single dataframe\n",
    "    prepared_dataframe = dataframe_samples[0]\n",
    "    for i in range(1,len(dataframe_samples)):\n",
    "        prepared_dataframe = pd.concat([prepared_dataframe, dataframe_samples[i]])\n",
    "    prepared_dataframe.reset_index(drop=True, inplace=True)\n",
    "    prepared_dataframe['context'].describe()\n",
    "    return prepared_dataframe\n",
    "\n",
    "sampled_dataframe = get_sampled_dataframe(emo_df_lst,'fear')\n",
    "speaker_prompts = sampled_dataframe.drop(columns = set(red_emo_prompt_df.columns) - set(['prompt', 'context']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output isolated emotion dataframes\n",
    "emo_to_num = {'joy': 0, 'anger': 1, 'disgust': 2, 'fear': 3,'trust': 4,'surprise': 5,'sadness': 6,'anticipation': 7}\n",
    "num_to_emo = {v: k for k, v in emo_to_num.items()}\n",
    "\n",
    "for i in range(len(emo_df_lst)):\n",
    "    emo_df_lst[i].to_csv('./useful_database_subsets/emotional_contexts/'+str(num_to_emo[i])+'_isolated_df.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd7d04f-09fd-4796-8191-9727f7d0953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: joy, prompt: A month back_comma_ I saved my drowning dog. I felt pleased with myself\n",
      "\n",
      "context: anger, prompt: i am bothered by my neighbors loud power tools next door\n",
      "\n",
      "context: disgust, prompt: i lied to my mother today \n",
      "\n",
      "context: fear, prompt: I had to go to the doctor recently to get something checked out_comma_ I was definitely nervous about the outcome.\n",
      "\n",
      "context: trust, prompt: I lent my car to my 17-year-old sister today. She left at 10 in the morning and it's midnight_comma_ and she still isn't back with it. I trust her mostly_comma_ but she hasn't been driving for long_comma_ and it makes me nervous thinking of her out there on the road.\n",
      "\n",
      "context: surprise, prompt: I can't believe there are so any good shows on television\n",
      "\n",
      "context: sadness, prompt: I cry every time I think of my grandmother.\n",
      "\n",
      "context: anticipation, prompt: MY kids are so different from each other. I think they are all going to do great things\n",
      "\n",
      "context: fear, prompt: I'm scared a tornado will come in and destroy my city\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(speaker_prompts['prompt'])):\n",
    "    print(f\"context: {speaker_prompts.loc[i,'context']}, prompt: {speaker_prompts.loc[i,'prompt']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d5aab53-6439-44f9-8c53-7ce40df152a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was sad when i couldnt go home on time'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sadness_df.sample(n=1).index\n",
    "sadness_df.loc[sadness_df.sample(n=1).index[0], 'prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "5a07c68f-e054-4584-97be-d55892c16838",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_prompts.to_csv('/useful_database_subsets/prompts_20.csv', index=None)\n",
    "idxlst = get_index_list(speaker_prompts, emo_df_lst)\n",
    "red_emo_prompt_df = remove_accepted_convos(idxlst,red_emo_prompt_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7cb63d33-6984-43b0-a761-86992b074bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, Index([15879], dtype='int64')], [1, Index([6512], dtype='int64')], [2, Index([5743], dtype='int64')], [3, Index([11351], dtype='int64')], [4, Index([575, 9767], dtype='int64')], [5, Index([3812], dtype='int64')], [6, Index([2567], dtype='int64')], [7, Index([10674], dtype='int64')], [0, Index([8297], dtype='int64')], [0, Index([5853], dtype='int64')], [1, Index([13516], dtype='int64')], [2, Index([5399], dtype='int64')], [3, Index([7434], dtype='int64')], [4, Index([13992], dtype='int64')], [5, Index([15122], dtype='int64')], [6, Index([5228], dtype='int64')], [7, Index([1004], dtype='int64')], [0, Index([15032], dtype='int64')], [0, Index([7275], dtype='int64')], [1, Index([11825], dtype='int64')], [2, Index([7628], dtype='int64')], [3, Index([3736], dtype='int64')], [4, Index([4592], dtype='int64')], [5, Index([1058], dtype='int64')], [6, Index([13485], dtype='int64')], [7, Index([2689], dtype='int64')], [0, Index([4413], dtype='int64')], [0, Index([14087], dtype='int64')], [1, Index([12235], dtype='int64')], [2, Index([1460], dtype='int64')], [3, Index([1508], dtype='int64')], [4, Index([6150], dtype='int64')], [5, Index([7295], dtype='int64')], [6, Index([15865], dtype='int64')], [7, Index([11005], dtype='int64')], [1, Index([956], dtype='int64')], [0, Index([9501], dtype='int64')], [1, Index([13623], dtype='int64')], [2, Index([5055], dtype='int64')], [3, Index([10024], dtype='int64')], [4, Index([3579], dtype='int64')], [5, Index([1127], dtype='int64')], [6, Index([16259], dtype='int64')], [7, Index([8504], dtype='int64')], [2, Index([15895], dtype='int64')], [0, Index([10052], dtype='int64')], [1, Index([15217], dtype='int64')], [2, Index([11384], dtype='int64')], [3, Index([12611], dtype='int64')], [4, Index([6190], dtype='int64')], [5, Index([8436], dtype='int64')], [6, Index([7875], dtype='int64')], [7, Index([11143], dtype='int64')], [3, Index([15924], dtype='int64')], [0, Index([15589], dtype='int64')], [1, Index([7255], dtype='int64')], [2, Index([13627], dtype='int64')], [3, Index([915, 9735], dtype='int64')], [4, Index([11850], dtype='int64')], [5, Index([13379], dtype='int64')], [6, Index([1937], dtype='int64')], [7, Index([437], dtype='int64')], [4, Index([16493], dtype='int64')], [0, Index([9161], dtype='int64')], [1, Index([9406], dtype='int64')], [2, Index([15353], dtype='int64')], [3, Index([9838], dtype='int64')], [4, Index([8054], dtype='int64')], [5, Index([5359, 9181], dtype='int64')], [6, Index([2048], dtype='int64')], [7, Index([13049], dtype='int64')], [5, Index([14901], dtype='int64')], [0, Index([2540], dtype='int64')], [1, Index([11878], dtype='int64')], [2, Index([15486], dtype='int64')], [3, Index([8613], dtype='int64')], [4, Index([12407], dtype='int64')], [5, Index([12278], dtype='int64')], [6, Index([14565], dtype='int64')], [7, Index([3319], dtype='int64')], [6, Index([12925], dtype='int64')], [0, Index([15413], dtype='int64')], [1, Index([11476], dtype='int64')], [2, Index([16309], dtype='int64')], [3, Index([16187], dtype='int64')], [4, Index([9455], dtype='int64')], [5, Index([3460], dtype='int64')], [6, Index([14033], dtype='int64')], [7, Index([9785], dtype='int64')], [0, Index([2107], dtype='int64')], [1, Index([9213], dtype='int64')], [2, Index([5047], dtype='int64')], [3, Index([3364], dtype='int64')], [4, Index([14805], dtype='int64')], [5, Index([1983], dtype='int64')], [6, Index([5083], dtype='int64')], [7, Index([6014], dtype='int64')], [1, Index([9216], dtype='int64')], [0, Index([2915], dtype='int64')], [1, Index([566], dtype='int64')], [2, Index([12542], dtype='int64')], [3, Index([5019], dtype='int64')], [4, Index([1542], dtype='int64')], [5, Index([6270], dtype='int64')], [6, Index([6212], dtype='int64')], [7, Index([6719], dtype='int64')], [2, Index([15856], dtype='int64')], [0, Index([6216], dtype='int64')], [1, Index([1247], dtype='int64')], [2, Index([3228], dtype='int64')], [3, Index([3687], dtype='int64')], [4, Index([1351], dtype='int64')], [5, Index([13929], dtype='int64')], [6, Index([581], dtype='int64')], [7, Index([3396], dtype='int64')], [3, Index([13675], dtype='int64')]]\n",
      "117\n",
      "7880\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[15879] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#remove_accepted_convos()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(red_emo_prompt_df))\n\u001b[0;32m---> 16\u001b[0m red_emo_prompt_df \u001b[38;5;241m=\u001b[39m remove_accepted_convos(idxlst,red_emo_prompt_df)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(red_emo_prompt_df))\n",
      "Cell \u001b[0;32mIn[233], line 16\u001b[0m, in \u001b[0;36mremove_accepted_convos\u001b[0;34m(index_list, base_df)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_accepted_convos\u001b[39m(index_list, base_df):\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m index_list:\n\u001b[0;32m---> 16\u001b[0m     base_df \u001b[38;5;241m=\u001b[39m base_df\u001b[38;5;241m.\u001b[39mdrop([index[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m base_df\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5345\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5346\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5347\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5348\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5349\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5350\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5351\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5352\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/indexes/base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[15879] not found in axis'"
     ]
    }
   ],
   "source": [
    "accepted_prompts = pd.DataFrame(columns=['context', 'prompt'])\n",
    "for i in range(13):\n",
    "    prompt_sample = pd.read_csv(current_dir + '/prompts_'+str(i+1)+'.csv')\n",
    "    accepted_prompts = pd.concat([accepted_prompts, prompt_sample])\n",
    "\n",
    "idxlst = get_index_list(accepted_prompts, emo_df_lst)\n",
    "\n",
    "print(idxlst)\n",
    "\n",
    "print(len(accepted_prompts))\n",
    "\n",
    "#remove_accepted_convos()\n",
    "\n",
    "print(len(red_emo_prompt_df))\n",
    "\n",
    "red_emo_prompt_df = remove_accepted_convos(idxlst,red_emo_prompt_df)\n",
    "\n",
    "print(len(red_emo_prompt_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306393f0-d720-4b97-8ce7-d6512e6ae62d",
   "metadata": {},
   "source": [
    "### Check the human accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80ce65df-4bb3-4af6-b480-2f431c2df5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19\n",
      "0.87\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import CEM as cem\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "human_df = pd.read_excel('group_1_final_ev.xlsx.ods', engine = 'odf')\n",
    "human_df\n",
    "\n",
    "person = \"Reviewer #1\"\n",
    "\n",
    "\n",
    "acc = accuracy_score(human_df['ROUND'], human_df[person])\n",
    "\n",
    "print(acc)\n",
    "\n",
    "\n",
    "human_df['ROUND_BINARY'] = human_df.apply(lambda x: 1 if (x['ROUND'] == 2 or x['ROUND'] == 1)  else 2, axis = 1)\n",
    "human_df['ROUND_BINARY'] = human_df.apply(lambda x: 1 if (x['ROUND'] == 2 or x['ROUND'] == 1)  else 2, axis = 1)\n",
    "\n",
    "human_df[person + '_BINARY'] = human_df.apply(lambda x: 1 if (x[person] == 2 or x[person] == 1)  else 2, axis = 1)\n",
    "\n",
    "acc = accuracy_score(human_df['ROUND_BINARY'], human_df[person + '_BINARY'])\n",
    "print(acc)\n",
    "human_df\n",
    "\n",
    "human_df.to_csv('human_binary.csv', index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5f1da-1cf7-4d4c-aa0b-cb23346f1221",
   "metadata": {},
   "source": [
    "### Turn a dataset to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "216c1324-e1bf-426a-9616-7b115bd7e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges_all_no_emo.csv'\n",
    "test_database_dir = '/processed_databases/EmpatheticExchanges/test.csv'\n",
    "\n",
    "train_database_dir = '/processed_databases/EmpatheticExchanges/'\n",
    "trainFile = current_dir + train_database_dir + 'EmpatheticExchanges_train.csv'\n",
    "testFile = current_dir + train_database_dir + 'EmpatheticExchanges_test.csv'\n",
    "df_train = pd.read_csv(trainFile)\n",
    "df_test = pd.read_csv(testFile)\n",
    "\n",
    "\n",
    "df_train['empathy_red'] = df_train.apply(lambda x: 2 if (x['empathy'] == 3 or x['empathy'] == 2)  else 1, axis = 1)\n",
    "df_train = df_train.drop(columns=['empathy'])\n",
    "df_train = df_train.rename(columns={\"empathy_red\": \"empathy\"})\n",
    "\n",
    "df_test['empathy_red'] = df_test.apply(lambda x: 2 if (x['empathy'] == 3 or x['empathy'] == 2)  else 1, axis = 1)\n",
    "df_test = df_test.drop(columns=['empathy'])\n",
    "df_test = df_test.rename(columns={\"empathy_red\": \"empathy\"})\n",
    "\n",
    "df_test.to_csv(current_dir + train_database_dir + 'EmpatheticExchanges_test_binary.csv', index = False)\n",
    "df_train.to_csv(current_dir + train_database_dir + 'EmpatheticExchanges_train_binary.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36427cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40987205693088047, 0.3907941555000378, 0.1993337875690817]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "608c10df",
   "metadata": {},
   "source": [
    "# Get most influential feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb6e6af5-eccc-4101-b0f9-cabe72046a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_feature_per_class_count(pattern_array, count_array, label,attribute_lst, desired_features): \n",
    "    #print(f'Exchange classified as {label}')\n",
    "\n",
    "    label -= 1\n",
    "\n",
    "    #print(f'Exchange classified as {label}')\n",
    "\n",
    "    #get the number of patterns that apply to this instance\n",
    "    counts = count_array[:,int(label)]\n",
    "    #get the patterns that cover this instance\n",
    "    patterns = pattern_array[counts.astype(np.bool_)]\n",
    "    #Slice the patterns into a list of item objects\n",
    "    pattern_items = [pattern.Items for pattern in patterns]\n",
    "\n",
    "    #check if they are the same\n",
    "    #print(f\"The number of patterns that cover this class is: {len(patterns)}\")\n",
    "    #print(f\"The number of pattern item lists for this class is: {len(pattern_items)}\")\n",
    "\n",
    "    pattern_features = []\n",
    "    for item_list in pattern_items:\n",
    "        single_pat_features = [item.Feature[0] for item in item_list]\n",
    "        #print(len(pat_features))\n",
    "        #print(len(set(pat_features)))\n",
    "        pattern_features.append(set(single_pat_features))\n",
    "\n",
    "\n",
    "    feature_count = [0] * len(attribute_lst)\n",
    "    feature_count_supports = [0] * len(attribute_lst)\n",
    "\n",
    "\n",
    "\n",
    "    for idx in range(len(pattern_features)):\n",
    "        #print(pattern_features)\n",
    "        for i in range(len(attribute_lst)):\n",
    "            if attribute_lst[i] in pattern_features[idx]:\n",
    "                feature_count[i] += 1\n",
    "                feature_count_supports[i] += 1*patterns[idx].Supports[label]\n",
    "\n",
    "\n",
    "\n",
    "    #print(feature_count)\n",
    "    #print(feature_count_supports)\n",
    "\n",
    "    sorted_lst = sorted([(val, idx) for (idx, val) in enumerate(feature_count)])\n",
    "    #print(sorted_lst)\n",
    "    #print(attribute_lst[sorted_lst[-1][1]])\n",
    "\n",
    "\n",
    "    #print('Most relevant features per pattern count')\n",
    "\n",
    "    feature_array = []\n",
    "\n",
    "\n",
    "    feature_array_speaker = []\n",
    "    feature_array_listener = []\n",
    "\n",
    "    for feature_num in range(1,desired_features+1):\n",
    "        #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        feature_array.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "\n",
    "    #print('Most relevant features related to speaker')\n",
    "\n",
    "    for feature_num in range(1,len(sorted_lst)):\n",
    "        if len(feature_array_speaker) < desired_features: \n",
    "            candidate_attribute = attribute_lst[sorted_lst[-feature_num][1]]\n",
    "            if candidate_attribute[:2] == 's_' or 'speaker' in str(candidate_attribute):\n",
    "                #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "                feature_array_speaker.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #print('Most relevant features related to listener')\n",
    "\n",
    "    for feature_num in range(1,len(sorted_lst)):\n",
    "        if len(feature_array_listener) < desired_features: \n",
    "            candidate_attribute = attribute_lst[sorted_lst[-feature_num][1]]\n",
    "            if not (candidate_attribute[:2] == 's_' or 'speaker' in str(candidate_attribute)):\n",
    "                #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "                feature_array_listener.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return feature_array, feature_array_listener, feature_array_speaker\n",
    "\n",
    "def find_relevant_feature_per_class_support(pattern_array, count_array, label,attribute_lst,desired_features): \n",
    "    #print(f'Exchange classified as {label}')\n",
    "\n",
    "    label -= 1\n",
    "\n",
    "    #get the number of patterns that apply to this instance\n",
    "    counts = count_array[:,int(label)]\n",
    "    #get the patterns that cover this instance\n",
    "    patterns = pattern_array[counts.astype(np.bool_)]\n",
    "    #Slice the patterns into a list of item objects\n",
    "    pattern_items = [pattern.Items for pattern in patterns]\n",
    "    \n",
    "    #for i in range(5):\n",
    "    #    print(patterns[i].Supports)\n",
    "\n",
    "    #check if they are the same\n",
    "\n",
    "    pattern_features = []\n",
    "    for item_list in pattern_items:\n",
    "        single_pat_features = [item.Feature[0] for item in item_list]\n",
    "        #print(len(pat_features))\n",
    "        #print(len(set(pat_features)))\n",
    "        pattern_features.append(set(single_pat_features))\n",
    "\n",
    "\n",
    "    feature_count = [0] * len(attribute_lst)\n",
    "    feature_count_supports = [0] * len(attribute_lst)\n",
    "\n",
    "\n",
    "    for idx in range(len(pattern_features)):\n",
    "        #print(pattern_features)\n",
    "        for i in range(len(attribute_lst)):\n",
    "            if attribute_lst[i] in pattern_features[idx]:\n",
    "                feature_count[i] += 1\n",
    "                feature_count_supports[i] += 1*patterns[idx].Supports[label]\n",
    "\n",
    "\n",
    "    sorted_lst = sorted([(val, idx) for (idx, val) in enumerate(feature_count_supports)])\n",
    "\n",
    "    #print('Most relevant features per pattern support')\n",
    "\n",
    "    feature_array = []\n",
    "    feature_array_speaker = []\n",
    "    feature_array_listener = []\n",
    "    for feature_num in range(1,desired_features+1):\n",
    "        #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        feature_array.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "\n",
    "    #print('Most relevant features related to speaker')\n",
    "\n",
    "    for feature_num in range(1,len(sorted_lst)):\n",
    "        if len(feature_array_speaker) < desired_features: \n",
    "            candidate_attribute = attribute_lst[sorted_lst[-feature_num][1]]\n",
    "            if candidate_attribute[:2] == 's_' or 'speaker' in str(candidate_attribute):\n",
    "                #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "                feature_array_speaker.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #print('Most relevant features related to listener')\n",
    "\n",
    "    for feature_num in range(1,len(sorted_lst)):\n",
    "        if len(feature_array_listener) < desired_features: \n",
    "            candidate_attribute = attribute_lst[sorted_lst[-feature_num][1]]\n",
    "            if not (candidate_attribute[:2] == 's_' or 'speaker' in str(candidate_attribute)):\n",
    "                #print(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "                feature_array_listener.append(attribute_lst[sorted_lst[-feature_num][1]])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return feature_array, feature_array_listener, feature_array_speaker\n",
    "\n",
    "\n",
    "def print_feature_with_values(exchange_df, feature_array):\n",
    "    for idx in range(len(exchange_df)):\n",
    "        #print(idx)\n",
    "        for item in feature_array:\n",
    "            print(f'{item} : {exchange_df.loc[idx,item]:.2f}')\n",
    "    return 0\n",
    "\n",
    "def get_feature_with_values(exchange_df, feature_array):\n",
    "    values = []\n",
    "    for idx in range(len(exchange_df)):\n",
    "        #print(idx)\n",
    "        for item in feature_array:\n",
    "            values.append((item,exchange_df.loc[idx,item]))\n",
    "            #print(f'{item} : {exchange_df.loc[idx,item]:.2f}')\n",
    "    return values\n",
    "\n",
    "    \n",
    "def get_most_relevant_features(classifier, exchange_df,criterion,attribute_lst, prediction):\n",
    "    #get most influential patterns\n",
    "    emerging_patterns = classifier.EmergingPatterns #access the patterns mined by the classifier\n",
    "    pattern_list = [] #patterns that cover the exchange\n",
    "    for instance in exchange_df.to_numpy(): \n",
    "        for pattern in emerging_patterns:\n",
    "            if pattern.IsMatch(instance):\n",
    "                pattern_list.append(pattern)   \n",
    "                #print(type(instance))\n",
    "    count_lst = [pattern.Counts for pattern in pattern_list]\n",
    "    pattern_arr = np.array(pattern_list)\n",
    "    #print(len(pattern_list))\n",
    "    count_arr = np.array(count_lst)\n",
    "    #print(len(count_arr))\n",
    "\n",
    "    #print(pattern_list[:5])\n",
    "\n",
    "\n",
    "    #print(f'Exchange classified as {prediction}')\n",
    "\n",
    "    #ifs important features by pattern support\n",
    "\n",
    "    if str(criterion) == 'support':\n",
    "        ifs, ifs_l, ifs_s = find_relevant_feature_per_class_support(pattern_arr,count_arr,prediction,attribute_lst,4)\n",
    "        return ifs, ifs_l, ifs_s \n",
    "    if str(criterion) == 'count':\n",
    "        ifc, ifc_l, ifc_s = find_relevant_feature_per_class_count(pattern_arr,count_arr,prediction,attribute_lst,4)\n",
    "        return ifc, ifc_l, ifc_s \n",
    "    else:\n",
    "        print('Invalid criterion, please select \"support\" to address the support patterns have over instances and \"count\" for just counting the times a feature appears in the patterns')\n",
    "        return [],[],[]\n",
    "\n",
    "\n",
    "def get_most_relevant_feature(classifier, exchange_df,role,attribute_lst, prediction):\n",
    "    #get most influential patterns\n",
    "    emerging_patterns = classifier.EmergingPatterns #access the patterns mined by the classifier\n",
    "    pattern_list = [] #patterns that cover the exchange\n",
    "    for instance in exchange_df.to_numpy(): \n",
    "        for pattern in emerging_patterns:\n",
    "            if pattern.IsMatch(instance):\n",
    "                pattern_list.append(pattern)   \n",
    "                #print(type(instance))\n",
    "    count_lst = [pattern.Counts for pattern in pattern_list]\n",
    "    pattern_arr = np.array(pattern_list)\n",
    "    #print(len(pattern_list))\n",
    "    count_arr = np.array(count_lst)\n",
    "    #print(len(count_arr))\n",
    "\n",
    "    #print(pattern_list[:5])\n",
    "\n",
    "    #print(f'Exchange classified as {prediction}')\n",
    "\n",
    "    #ifs important features by pattern support\n",
    "    ifs, ifs_l, ifs_s = find_relevant_feature_per_class_support(pattern_arr,count_arr,prediction,attribute_lst,4)\n",
    "\n",
    "    print()\n",
    "\n",
    "    #ifc important features by pattern count (how many times they appear in the patterns that cover the instance)\n",
    "    ifc, ifc_l, ifc_s = find_relevant_feature_per_class_count(pattern_arr,count_arr,prediction,attribute_lst,4)\n",
    " \n",
    "    #print(exchange_df)\n",
    "    print('Most relevant features per pattern support')\n",
    "    print_feature_with_values(exchange_df,ifs)\n",
    "    print('Most relevant support features related to listener')\n",
    "    print_feature_with_values(exchange_df,ifs_l)\n",
    "    print('Most relevant support features related to speaker')\n",
    "    print_feature_with_values(exchange_df,ifs_s)\n",
    "\n",
    "\n",
    "\n",
    "    return ifs, ifc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5026da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant features per pattern support\n",
      "l_word_len : 10.00\n",
      "l_positive : 0.00\n",
      "dominance_speaker : -0.03\n",
      "s_word_len : 13.00\n",
      "[('l_word_len', 10), ('l_positive', 0.0046051578), ('dominance_speaker', -0.03280000000000001), ('s_word_len', 13)]\n",
      "Most relevant support features related to listener\n",
      "l_word_len : 10.00\n",
      "l_positive : 0.00\n",
      "l_negative : 0.93\n",
      "valence_listener : 0.42\n",
      "[('l_word_len', 10), ('l_positive', 0.0046051578), ('l_negative', 0.9260325), ('valence_listener', 0.4166666666666667)]\n",
      "Most relevant support features related to speaker\n",
      "dominance_speaker : -0.03\n",
      "s_word_len : 13.00\n",
      "arousal_speaker : 0.29\n",
      "s_negative : 0.96\n",
      "[('dominance_speaker', -0.03280000000000001), ('s_word_len', 13), ('arousal_speaker', 0.2896), ('s_negative', 0.9551038)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'Super sad today. It is the weekend and I have a hard time with loneliness on the weekends especially.', \"I love the weekends actually\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Aww man, that's not ideal Did you get the plates?\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'I hate when my wife and son are away from me', \"Aww that is sweet You are a good dad\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'My little cousin  was nice and gave me a present!', \"Aww cool! was the ocasion special?\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadequate\", \"Yeah, it really sucks loneliness is no easy thing to go though\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadequate\", \"Yeah, you are \",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"Does it bother you when your friends have all dates and you're single? It makes me feel inadequate\", \"Oh yeah, it bothers me a lot too! \",model_components)\n",
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Aww man, that's not ideal Did you get the plates?\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"That's what you get haha!\",model_components)\n",
    "#processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,\"I was so mad earlier someone hit my car and just drove off!\", \"Was it a bad accident?\",model_components)\n",
    "\n",
    "#print(y_pred)\n",
    "#print(processed_exchange)\n",
    "\n",
    "influential,influential_l,influential_s = get_most_relevant_features(pbc, processed_exchange,'support',att_lst, y_pred)\n",
    "\n",
    "print('Most relevant features per pattern support')\n",
    "print_feature_with_values(processed_exchange,influential)\n",
    "print(get_feature_with_values(processed_exchange,influential))\n",
    "print('Most relevant support features related to listener')\n",
    "print_feature_with_values(processed_exchange,influential_l)\n",
    "print(get_feature_with_values(processed_exchange,influential_l))\n",
    "print('Most relevant support features related to speaker')\n",
    "print_feature_with_values(processed_exchange,influential_s)\n",
    "print(get_feature_with_values(processed_exchange,influential_s))\n",
    "\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fdf8d97-65fa-45f2-86a8-53b4fad23c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most relevant features for classification\n",
      "[('l_word_len', 3), ('s_word_len', 8), ('arousal_speaker', 0.24066666666666667)]\n",
      "Most relevant features from the listener\n",
      "[('l_negative', 0.2412955), ('encouraging', 2.877417682611849e-05), ('questioning', 0.9979150891304016)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most relevant features for classification\n",
      "[('l_word_len', 7), ('l_neutral', 0.5941971), ('s_word_len', 18)]\n",
      "Most relevant features from the listener\n",
      "[('l_neutral', 0.5941971), ('suggesting', 9.872257214738056e-05), ('l_negative', 0.375899)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most relevant features for classification\n",
      "[('l_word_len', 6), ('s_word_len', 11), ('arousal_speaker', -0.11533333333333333)]\n",
      "Most relevant features from the listener\n",
      "[('l_neutral', 0.21651484), ('encouraging', 7.498222112189978e-05), ('neutral', 0.00021213227591942996)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most relevant features for classification\n",
      "[('l_word_len', 5), ('sympathizing', 0.0006214195163920522), ('s_word_len', 6)]\n",
      "Most relevant features from the listener\n",
      "[('sympathizing', 0.0006214195163920522), ('l_neutral', 0.466616), ('encouraging', 0.0001263265876332298)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persona = ['I was really nervous to move across country.','Knew no one where we were moving_comma_ and also far away from my mother_comma_ who is getting old.', 'oh sorry_comma_ we knew no one where we were moving to', 'Amazingly hehe. But here I am.'] \n",
    "personb = ['why were you?','no one knew that you were moving?','oh_comma_ that has to be scary', 'here you are_comma_ killing it ']\n",
    "print('')\n",
    "\n",
    "for i in range(4):\n",
    "    print('', end = '')\n",
    "    speaker_uttearance = persona[i]\n",
    "    listener_uttearance = personb[i]\n",
    "    processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,speaker_uttearance,listener_uttearance,model_components)\n",
    "    #print(y_pred)\n",
    "    print()\n",
    "    influential,influential_l,influential_s = get_most_relevant_features(pbc, processed_exchange,'support',att_lst, y_pred)\n",
    "    print('Most relevant features for classification')\n",
    "    #print_feature_with_values(processed_exchange,influential)\n",
    "    print(get_feature_with_values(processed_exchange,influential)[:3])\n",
    "    print('Most relevant features from the listener')\n",
    "    print(get_feature_with_values(processed_exchange,influential_l)[:3])\n",
    "    print('', end = '\\r')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6bf0a",
   "metadata": {},
   "source": [
    "## Testing changes in top features using new VA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dae9f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 2, 2, 3, 3]\n",
      "[2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 3, 1, 3, 2, 1, 2, 3, 2, 1, 3]\n",
      "[3, 3, 1, 2, 3, 3, 1, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 1, 3]\n",
      "0.5\n",
      "0.6\n",
      "12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst,'I hate when my wife and son are away from me', \"I get that you're feeling bad but do not let it get to you. I'm sure you'll be extra happy once they are here\",model_components)\n",
    "\n",
    "columns_of_processed_exchange = processed_exchange.columns\n",
    "\n",
    "#print(columns_of_processed_exchange)\n",
    "\n",
    "conversations = ['body', 'comic', 'faith', 'joy', 'lottery', 'manager', 'racoon', 'sister', 'morning', 'furiosa']\n",
    "\n",
    "#conversations = ['body', 'comic', 'joy', 'lottery', 'racoon', 'sister', 'morning', 'furiosa']\n",
    "\n",
    "video_weight = 0.2\n",
    "\n",
    "\n",
    "#conversations =  ['faith']\n",
    "empathy_truth = []\n",
    "predictions_text = []\n",
    "predictions_video = []\n",
    "\n",
    "\n",
    "for i in range(len(conversations)):\n",
    "    convo_idx = i\n",
    "    convos_to_test = pd.read_csv(current_dir + '/useful_database_subsets/video_exchanges/convos_to_test.csv')\n",
    "\n",
    "    #print(convos_to_test.columns)\n",
    "\n",
    "    df_convo = convos_to_test[convos_to_test['conversation'] == conversations[convo_idx]].reset_index()\n",
    "\n",
    "    #print(convos_to_test[convos_to_test['conversation'] == conversations[convo_idx]])\n",
    "\n",
    "\n",
    "    string_arr = [\n",
    "                [df_convo.loc[0,'speaker_utterance'],df_convo.loc[0,'listener_utterance']],\n",
    "                [df_convo.loc[1,'speaker_utterance'],df_convo.loc[1,'listener_utterance']]\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "    exchanges_df = pd.DataFrame(columns=columns_of_processed_exchange)\n",
    "\n",
    "    for ex in string_arr:\n",
    "        #print(ex)\n",
    "        single_exchange, y_pred  = exchange_processer.predict_exchange_empathy(pbc, flag_array, 1, att_lst, ex[0], ex[1],model_components)\n",
    "        single_exchange['pred_text'] = y_pred\n",
    "        influential,influential_l,influential_s = get_most_relevant_features(pbc, single_exchange,'support',att_lst, y_pred)     \n",
    "        single_exchange['most_influential_before'] = str(get_feature_with_values(single_exchange,influential))\n",
    "        single_exchange['most_influential_before_l'] = str(get_feature_with_values(single_exchange,influential_l))\n",
    "        single_exchange['most_influential_before_s'] = str(get_feature_with_values(single_exchange,influential_s))\n",
    "        exchanges_df = pd.concat([exchanges_df, single_exchange])\n",
    "        #print(single_exchange)\n",
    "\n",
    "    exchanges_df = exchanges_df.reset_index(drop= True)\n",
    "\n",
    "    #print(exchanges_df)\n",
    "\n",
    "    conversation = []\n",
    "    video_av_values = pd.read_csv(current_dir + '/useful_database_subsets/video_exchanges/exchanges/' + 'exchanges_'+str(conversations[convo_idx])+'.csv')\n",
    "    exchanges_df['valence_speaker'] = (1-video_weight)*exchanges_df['valence_speaker'] + (video_weight)*video_av_values['valence_right']\n",
    "    exchanges_df['arousal_speaker'] = (1-video_weight)*exchanges_df['arousal_speaker'] + (video_weight)*video_av_values['arousal_right'] \n",
    "    exchanges_df['valence_listener'] = (1-video_weight)*exchanges_df['valence_listener'] + (video_weight)*video_av_values['valence_left']\n",
    "    exchanges_df['arousal_listener'] = (1-video_weight)*exchanges_df['arousal_listener'] + (video_weight)*video_av_values['arousal_left'] \n",
    "\n",
    "    #mimicry\n",
    "    exchanges_df['emotional_similarity'] = exchanges_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "    exchanges_df['mimicry'] = exchanges_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "    exchanges_df = exchanges_df.drop(columns = ['emotional_similarity'])\n",
    "\n",
    "\n",
    "    exchanges_df['pred_video'] = pbc.predict(exchanges_df)\n",
    "    exchanges_df['pred_video'] = exchanges_df['pred_video'] + 1\n",
    "    exchanges_df['new_empathy_reduced'] = df_convo['new_empathy_reduced']\n",
    "    exchanges_df['empathy'] = df_convo['empathy']\n",
    "\n",
    "    for i in range(len(exchanges_df)):\n",
    "        #print(columns_of_processed_exchange)\n",
    "        single_exchange = exchanges_df.iloc[[i]].drop(columns=['most_influential_before','most_influential_before_l','most_influential_before_s']).reset_index()\n",
    "        mia,mial,mias = get_most_relevant_features(pbc,single_exchange,'support',att_lst, y_pred)       \n",
    "        \n",
    "        exchanges_df.loc[i,'most_influential_after'] = str(get_feature_with_values(single_exchange,mia))\n",
    "        exchanges_df.loc[i,'most_influential_after_l'] = str(get_feature_with_values(single_exchange,mial))\n",
    "        exchanges_df.loc[i,'most_influential_after_s'] = str(get_feature_with_values(single_exchange,mias))\n",
    "\n",
    "    exchanges_df.to_csv(current_dir + '/useful_database_subsets/video_exchanges/'+'exchange_predictions_'+str(conversations[convo_idx])+'.csv')\n",
    "\n",
    "    \n",
    "\n",
    "    empathy_truth.append(exchanges_df.loc[0,'new_empathy_reduced'])\n",
    "    empathy_truth.append(exchanges_df.loc[1,'new_empathy_reduced'])\n",
    "    predictions_text.append(exchanges_df.loc[0,'pred_text'])\n",
    "    predictions_text.append(exchanges_df.loc[1,'pred_text'])\n",
    "    predictions_video.append(exchanges_df.loc[0,'pred_video'])\n",
    "    predictions_video.append(exchanges_df.loc[1,'pred_video'])\n",
    "\n",
    "for i in range(len(predictions_text)):\n",
    "    predictions_text[i] = predictions_text[i].astype(np.int64)\n",
    "\n",
    "print(empathy_truth)\n",
    "print(predictions_text)\n",
    "print(predictions_video)\n",
    "\n",
    "acc_text = accuracy_score(empathy_truth, predictions_text)\n",
    "acc_video = accuracy_score(empathy_truth, predictions_video)\n",
    "\n",
    "print(acc_text)\n",
    "print(acc_video)\n",
    "\n",
    "for i in range(len(empathy_truth)):\n",
    "    if empathy_truth[i] != predictions_video[i] and empathy_truth[i] == predictions_text[i]:\n",
    "        print(i+1)    \n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92c98e",
   "metadata": {},
   "source": [
    "## Get experiment order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c25602db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 7, 8, 1, 5, 2, 3)\n",
      "(7, 8, 6, 4, 2, 3, 1, 5)\n",
      "(3, 2, 5, 1, 8, 7, 4, 6)\n",
      "(1, 5, 2, 3, 6, 4, 7, 8)\n",
      "(5, 1, 3, 2, 4, 6, 8, 7)\n",
      "(2, 3, 1, 5, 7, 8, 6, 4)\n",
      "(4, 6, 8, 7, 5, 1, 3, 2)\n",
      "(8, 7, 4, 6, 3, 2, 5, 1)\n",
      "True\n",
      "[('sadness', 'low'), ('joy', 'high'), ('surprise', 'medium'), ('trust', 'high'), ('anger', 'low'), ('anticipation', 'low'), ('disgust', 'high'), ('fear', 'high')]\n",
      "[('surprise', 'high'), ('trust', 'low'), ('sadness', 'high'), ('joy', 'high'), ('disgust', 'medium'), ('fear', 'high'), ('anger', 'high'), ('anticipation', 'high')]\n",
      "[('fear', 'high'), ('disgust', 'high'), ('anticipation', 'medium'), ('anger', 'high'), ('trust', 'medium'), ('surprise', 'medium'), ('joy', 'low'), ('sadness', 'medium')]\n",
      "[('anger', 'high'), ('anticipation', 'medium'), ('disgust', 'low'), ('fear', 'medium'), ('sadness', 'medium'), ('joy', 'high'), ('surprise', 'high'), ('trust', 'medium')]\n",
      "[('anticipation', 'low'), ('anger', 'high'), ('fear', 'high'), ('disgust', 'medium'), ('joy', 'high'), ('sadness', 'high'), ('trust', 'high'), ('surprise', 'medium')]\n",
      "[('disgust', 'medium'), ('fear', 'low'), ('anger', 'low'), ('anticipation', 'high'), ('surprise', 'medium'), ('trust', 'low'), ('sadness', 'high'), ('joy', 'low')]\n",
      "[('joy', 'low'), ('sadness', 'high'), ('trust', 'high'), ('surprise', 'low'), ('anticipation', 'low'), ('anger', 'medium'), ('fear', 'high'), ('disgust', 'high')]\n",
      "[('trust', 'low'), ('surprise', 'medium'), ('joy', 'low'), ('sadness', 'low'), ('fear', 'medium'), ('disgust', 'high'), ('anticipation', 'high'), ('anger', 'high')]\n",
      "                     0                     1                       2  \\\n",
      "0       (sadness, low)      (surprise, high)            (fear, high)   \n",
      "1          (joy, high)          (trust, low)         (disgust, high)   \n",
      "2   (surprise, medium)       (sadness, high)  (anticipation, medium)   \n",
      "3        (trust, high)           (joy, high)           (anger, high)   \n",
      "4         (anger, low)     (disgust, medium)         (trust, medium)   \n",
      "5  (anticipation, low)          (fear, high)      (surprise, medium)   \n",
      "6      (disgust, high)         (anger, high)              (joy, low)   \n",
      "7         (fear, high)  (anticipation, high)       (sadness, medium)   \n",
      "\n",
      "                        3                    4                     5  \\\n",
      "0           (anger, high)  (anticipation, low)     (disgust, medium)   \n",
      "1  (anticipation, medium)        (anger, high)           (fear, low)   \n",
      "2          (disgust, low)         (fear, high)          (anger, low)   \n",
      "3          (fear, medium)    (disgust, medium)  (anticipation, high)   \n",
      "4       (sadness, medium)          (joy, high)    (surprise, medium)   \n",
      "5             (joy, high)      (sadness, high)          (trust, low)   \n",
      "6        (surprise, high)        (trust, high)       (sadness, high)   \n",
      "7         (trust, medium)   (surprise, medium)            (joy, low)   \n",
      "\n",
      "                     6                     7  \n",
      "0           (joy, low)          (trust, low)  \n",
      "1      (sadness, high)    (surprise, medium)  \n",
      "2        (trust, high)            (joy, low)  \n",
      "3      (surprise, low)        (sadness, low)  \n",
      "4  (anticipation, low)        (fear, medium)  \n",
      "5      (anger, medium)       (disgust, high)  \n",
      "6         (fear, high)  (anticipation, high)  \n",
      "7      (disgust, high)         (anger, high)  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "plutchik_emo_dic = {0:'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'anticipation', 5: 'sadness', 6: 'surprise' , 7: 'trust'}\n",
    "empathy_level_dic = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "\n",
    "# this only works for Iterable[Iterable]\n",
    "def is_latin_rectangle(rows):\n",
    "    valid = True\n",
    "    for row in rows:\n",
    "        if len(set(row)) < len(row):\n",
    "            valid = False\n",
    "    if valid and rows:\n",
    "        for i, val in enumerate(rows[0]):\n",
    "            col = [row[i] for row in rows]\n",
    "            if len(set(col)) < len(col):\n",
    "                valid = False\n",
    "                break\n",
    "    return valid\n",
    "\n",
    "def is_latin_square(rows):\n",
    "    return is_latin_rectangle(rows) and len(rows) == len(rows[0])\n",
    "\n",
    "# : prepare the input\n",
    "n = 8\n",
    "items = list(range(1, n + 1))\n",
    "# shuffle items\n",
    "random.shuffle(items)\n",
    "# number of permutations\n",
    "\n",
    "\n",
    "\n",
    "def latin_square(items, shuffle=True):\n",
    "    result = []\n",
    "    for elems in itertools.permutations(items):\n",
    "        valid = True\n",
    "        for i, elem in enumerate(elems):\n",
    "            orthogonals = [x[i] for x in result] + [elem]\n",
    "            if len(set(orthogonals)) < len(orthogonals):\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            result.append(elems)\n",
    "    if shuffle:\n",
    "        random.shuffle(result)\n",
    "    return result\n",
    "\n",
    "ltn_sq_1 = latin_square(items)\n",
    "for row in ltn_sq_1:\n",
    "    print(row)\n",
    "print(is_latin_square(ltn_sq_1))\n",
    "\n",
    "type(ltn_sq_1)\n",
    "\n",
    "experiment_squares = []\n",
    "\n",
    "#print(experiment_squares[0][1])\n",
    "\n",
    "for i in range(n):\n",
    "    experiment_squares.append(list(ltn_sq_1[i]))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        experiment_squares[i][j] = (plutchik_emo_dic[experiment_squares[i][j]-1], random.choice(['low','medium','high']))\n",
    "\n",
    "\n",
    "for row in experiment_squares:\n",
    "    print(row)\n",
    "\n",
    "\n",
    "#for item in experiment_squares: \n",
    "#    print(item)\n",
    "\n",
    "latin_sqr_df = pd.DataFrame(experiment_squares).T\n",
    "\n",
    "\n",
    "print(latin_sqr_df)\n",
    "\n",
    "number_of_square = 2\n",
    "\n",
    "latin_sqr_df.to_csv('./useful_database_subsets/experiment_square_'+str(number_of_square)+'.csv')\n",
    "\n",
    "#random.choice(['low','medium','high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04d5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
