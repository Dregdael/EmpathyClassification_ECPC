{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d0a22c-4c57-42ec-9fd1-acccab9a2cc7",
   "metadata": {},
   "source": [
    "# DEMO: Empathy classification using a pattern classifier\n",
    "\n",
    "In this notebook, it is possible to use a previously trained contrast-pattern classification algorithm to obtain the empathy level of a conversation between two people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7d243c2-35f2-4a20-a573-6142fc14b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 17:53:41.936391: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 17:53:42.048089: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 17:53:42.048136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 17:53:42.064822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 17:53:42.099483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 17:53:42.743037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#import train_classifier as trainer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#import test_classifier as tester\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatabase_processing_package\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata_processer\u001b[39;00m\n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/database_processing_package.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#epitome \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#from classifiers.epitome_mechanisms import epitome_predictor as epitome\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mepitome_mechanisms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m epitome_predictor \u001b[38;5;28;01mas\u001b[39;00m epitome\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnrc_vad_lexicon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lexicon_analysis \u001b[38;5;28;01mas\u001b[39;00m lexicon\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#emotion\u001b[39;00m\n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/classifiers/nrc_vad_lexicon/lexicon_analysis.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#To obtain lemmatize words\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "#import classifier\n",
    "#from PBC4cip import PBC4cip\n",
    "#from PBC4cip.core.Evaluation import obtainAUCMulticlass\n",
    "#from PBC4cip.core.Helpers import get_col_dist, get_idx_val\n",
    "\n",
    "#utilities for database management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "#import train_classifier as trainer\n",
    "#import test_classifier as tester\n",
    "#import database_processing_package as data_processer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ab96-e4aa-47e5-b4f4-5face72f158e",
   "metadata": {},
   "source": [
    "First, we will obtain the model and sample conversation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74aab462-ebf7-495c-ba30-168b3f718f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haru/EmpathyClassification_ECPC/Experiments/outputs/Experiment 25/trained_pbc4cip.sav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Man, I let one of my friends take my Benz one day to run some errands. I really thought she would be careful with it.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd() #get directory of the repository\n",
    "database_dir = '/processed_databases/EmpatheticExchanges'\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(25) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "print(model_directory)\n",
    "\n",
    "database = pd.read_csv(current_dir + database_dir + '/EmpatheticExchanges.csv')\n",
    "\n",
    "starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "starting_exchange_db = starting_exchange_db.reset_index()\n",
    "len_of_db = len(starting_exchange_db)\n",
    "index_of_sample = random.randint(0, len_of_db)\n",
    "sample = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "sample = re.sub(\"_comma_\", ',', sample)\n",
    "database \n",
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "963666b4-3915-4a26-b780-b3d98d972397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide your response:  Oh no, why?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oh no, why?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag = True\n",
    "while(flag):\n",
    "    answer = input(\"Provide your response: \")\n",
    "    if answer.lower() == '':\n",
    "        print('No answer received, please provide a response')\n",
    "    else:\n",
    "        flag = False\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "427849ee-3c1d-4775-80b9-3a59b8e70732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "conv_id\n",
      "context\n",
      "prompt\n",
      "speaker_utterance\n",
      "listener_utterance\n",
      "exchange_number\n",
      "s_negative\n",
      "s_neutral\n",
      "s_positive\n",
      "l_negative\n",
      "l_neutral\n",
      "l_positive\n",
      "predictions_ER\n",
      "predictions_IP\n",
      "predictions_EX\n",
      "s_word_len\n",
      "l_word_len\n",
      "agreeing\n",
      "acknowledging\n",
      "encouraging\n",
      "consoling\n",
      "sympathizing\n",
      "suggesting\n",
      "questioning\n",
      "wishing\n",
      "neutral\n",
      "mimicry\n",
      "empathy\n"
     ]
    }
   ],
   "source": [
    "dataframe_columns = starting_exchange_db.columns\n",
    "for i in dataframe_columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec5a65-79b1-4b7f-8bc2-aa41accadd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
