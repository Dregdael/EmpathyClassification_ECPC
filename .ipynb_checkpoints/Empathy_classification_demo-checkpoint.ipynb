{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d0a22c-4c57-42ec-9fd1-acccab9a2cc7",
   "metadata": {},
   "source": [
    "# DEMO: Empathy classification using a pattern classifier\n",
    "\n",
    "In this notebook, it is possible to use a previously trained contrast-pattern classification algorithm to obtain the empathy level of a conversation between two people. \n",
    "\n",
    "A conversation prompt is presented, pulled from the EmpatheticExchanges database subset for testing classification algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c48750-894d-4cfa-b690-743951e0d5e6",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "This subsection focuses on setting up the environment, functions, utilities, and models required for the demo. Likewise, it is where the variables are manually declared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d243c2-35f2-4a20-a573-6142fc14b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "#import classifier\n",
    "from PBC4cip import PBC4cip\n",
    "from PBC4cip.core.Evaluation import obtainAUCMulticlass\n",
    "from PBC4cip.core.Helpers import get_col_dist, get_idx_val\n",
    "\n",
    "#utilities for database management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import train_classifier as trainer\n",
    "import test_classifier as tester\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "#relevant classifiers for annotating exchange feature\n",
    "from classifiers.empathetic_intent import intent_prediction as ip\n",
    "from classifiers.sentiment import sentiment_prediction as sp\n",
    "from classifiers.epitome_mechanisms import epitome_predictor as epitome\n",
    "from classifiers.nrc_vad_lexicon import lexicon_analysis as lexicon\n",
    "from classifiers.course_grained_emotion import pretrained_32emotions as em32\n",
    "from classifiers.course_grained_emotion import emotion_reductor as em_red\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "from spellchecker import SpellChecker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ab96-e4aa-47e5-b4f4-5face72f158e",
   "metadata": {},
   "source": [
    "### Selection of features\n",
    "\n",
    "In this cell, we define the model that will be used for this task. We declare its location directory, and its name \"trained_pbc4cip.sav\" \n",
    "\n",
    "Likewise, we declare a \"feature vector\" which contains binary flags for the features used by the model to predict empathy. \n",
    "\n",
    "Finally, we declare the database from which the prompts will be extracted from by declaring its directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aab462-ebf7-495c-ba30-168b3f718f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant directories\n",
    "current_dir = os.getcwd() #get directory of the repository\n",
    "#Select an appropriate classification model in the Experiments folder\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "\n",
    "feature2number = {'database_to_classify':0,'intent' : 1, 'sentiment' : 2, 'epitome':3, 'VAD_vectors':4, 'utterance_length':5,\n",
    "                  '32_emotion_labels':6,'20_emotion_labels':7, \n",
    "                  '8_emotion_labels':8, 'emotion_mimicry':9, 'Reduce_empathy_labels':10, \n",
    "                  'exchange_number' : 11}\n",
    "\n",
    "feature_vector = [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
    "'''\n",
    "                 [1,#database to pull from 0 = empatheticconversations (old), 1 empatheticexchanges (new)\n",
    "                  1,#intent\n",
    "                  1,#sentiment\n",
    "                  0,#epitome\n",
    "                  1,#vad lexicon\n",
    "                  1,#length\n",
    "                  0,#emotion 32\n",
    "                  0,#emotion 20\n",
    "                  1,#emotion 8\n",
    "                  1,#emotion mimicry\n",
    "                  1, #reduce empathy labels\n",
    "                  1 #exchange number\n",
    "                  ]\n",
    "'''\n",
    "\n",
    "if feature_vector[feature2number['database_to_classify']] == 1: \n",
    "    database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges_test.csv'\n",
    "else: \n",
    "    database_dir = '/processed_databases/EmpatheticConversationsExchangeFormat/EmpatheticConversations_ex.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548e12f-e577-4f23-b177-d4273c9def5b",
   "metadata": {},
   "source": [
    "### Loading classification models\n",
    "\n",
    "In this cell, we prepare the classification models for obtaining empathy-related features. These models must be pretrained before they are loaded by this demo. \n",
    "\n",
    "### WARNING: DO NOT RUN THIS TWICE. It will cause memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6768f2-4db7-4bb2-87b7-d0b7b391a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load intent model\n",
    "if feature_vector[feature2number['intent']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    model_intent,tokenizer_intent,device = ip.loadModelTokenizerAndDevice(empIntSubDir) #get model and parameters\n",
    "#load sentiment model\n",
    "if feature_vector[feature2number['sentiment']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    sent_model, sent_tokenzr = sp.loadSentimentModel() #get model and tokenizer\n",
    "#epitome model is loaded during inference due to the code of its classifier\n",
    "if feature_vector[feature2number['epitome']] == 1:\n",
    "    epitome_empathy_classifier = epitome.load_epitome_classifier('classifiers/epitome_mechanisms/trained_models')\n",
    "#load lexicon\n",
    "if feature_vector[feature2number['VAD_vectors']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')\n",
    "#load emotion classifier with 32 labels for any of the emotion labels options\n",
    "if (feature_vector[feature2number['32_emotion_labels']] == 1) or (feature_vector[feature2number['20_emotion_labels']] == 1) or (feature_vector[feature2number['8_emotion_labels']] == 1):\n",
    "    emo32_model, emo32_tokenzr = em32.load32EmotionsModel() #get model and tokenizer\n",
    "#it is necessary to get the VAD vectors for obtaining emotion mimicry\n",
    "if feature_vector[feature2number['emotion_mimicry']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08573e51-15fc-4461-b8a6-65f7ca8a13a7",
   "metadata": {},
   "source": [
    "### Definition of data processing function\n",
    "\n",
    "This is a function used to transform a text exchange into the format necessary for classification. It adds the following features: \n",
    "\n",
    "* Sentiment\n",
    "* EPITOME mechanisms (Sharma, 2019)\n",
    "* Valence, Arousal, and Dominance emotion vectors\n",
    "* Utterance lengths for both participants\n",
    "* Emotion labels\n",
    "* Empathetic Intent\n",
    "* Whether there is emotion mimicry\n",
    "\n",
    "This features are dependent on the feature vector defined at the start of this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0a2ab89d-2d9d-433e-92d4-43c7f1de60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(sample_df,control_vector):\n",
    "    print('processing data....')\n",
    "    if control_vector[feature2number['sentiment']] == 1: \n",
    "        sample_df['speaker_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'speaker_utterance')) \n",
    "        sample_df[['s_negative','s_neutral', 's_positive']] = pd.DataFrame(sample_df.speaker_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df['listener_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'listener_utterance')) \n",
    "        sample_df[['l_negative','l_neutral', 'l_positive']] = pd.DataFrame(sample_df.listener_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['speaker_sentiment','listener_sentiment'])\n",
    "    if control_vector[feature2number['epitome']] == 1:\n",
    "        sample_df = epitome.classify_epitome_values(epitome_empathy_classifier, sample_df)\n",
    "    if control_vector[feature2number['VAD_vectors']] == 1:\n",
    "        sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "        sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])\n",
    "    if control_vector[feature2number['utterance_length']] == 1:\n",
    "        sample_df['s_word_len'] = sample_df['speaker_utterance'].apply(data_processer.get_word_len) \n",
    "        sample_df['l_word_len'] = sample_df['listener_utterance'].apply(data_processer.get_word_len) \n",
    "    if (control_vector[feature2number['32_emotion_labels']] == 1) or (control_vector[feature2number['20_emotion_labels']] == 1) or (control_vector[feature2number['8_emotion_labels']] == 1):\n",
    "        sample_df['speaker_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'speaker_utterance')) \n",
    "        sample_df['listener_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'listener_utterance')) \n",
    "        if (control_vector[feature2number['20_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels('listener_emotion',sample_df)\n",
    "        if (control_vector[feature2number['8_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('listener_emotion',sample_df)\n",
    "    if control_vector[feature2number['intent']] == 1: \n",
    "        sample_df['utterance'] = str(answer)\n",
    "        sample_df['is_response'] = 1\n",
    "        sample_df['empathetic_intent'] = sample_df.apply(data_processer.get_emp_intent_probabilities, axis=1, args = (model_intent,tokenizer_intent,device,'listener_utterance'))\n",
    "        sample_df[data_processer.intent_labels] = pd.DataFrame(sample_df.empathetic_intent.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['empathetic_intent','utterance','is_response'])\n",
    "    if control_vector[feature2number['emotion_mimicry']] == 1:\n",
    "        if(control_vector[4] == 1):\n",
    "            #get the emotional similarity, if it is more than 0.7 set mimicry to 1\n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns = ['emotional_similarity'])\n",
    "        else: \n",
    "            sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds,spll)) \n",
    "            sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds,spll)) \n",
    "            sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "            sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "            sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])                \n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns =  ['valence_speaker','arousal_speaker','dominance_speaker','valence_listener','arousal_listener','dominance_listener','emotional_similarity'])\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('category')\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('string')\n",
    "        #sample_df = sample_df.drop(columns =  ['predictions_EX'])\n",
    "    print('done')\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53078ccd-bfa0-4c1c-8a7d-b9c4572d2008",
   "metadata": {},
   "source": [
    "### database setup\n",
    "\n",
    "We load the database. Next, we filter it to have only samples that start a conversation. This is done by selecting those that have an \"exchange_number\" variable of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5739e968-0026-46a3-80cf-8d9ec1ed07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(current_dir + database_dir)\n",
    "\n",
    "starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "starting_exchange_db = starting_exchange_db.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78d7e6-b49d-4268-9b9f-41f665cb382e",
   "metadata": {},
   "source": [
    "### Load our classification model\n",
    "\n",
    "In this cell, we run the empathy classification model that we have previously trained. The model selection is done through specifying the directory in which the model was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec3862b-594f-4d71-bbf2-c3fc7c672abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "pbc = pickle.load(open(model_directory, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4fd79-91cd-47cf-bfc8-8aac6c569646",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "In this subsection, we present that working parts of the demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8509-f8a2-49ba-99ba-5ae0b971e6a4",
   "metadata": {},
   "source": [
    "### Conversation starter\n",
    "\n",
    "We randomly sample the database for a conversation prompt. This is equivalent to an utterance of a first agent, to which we will provide a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb94567e-25c0-4522-8f1e-a68816831299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"My daughter was in a ballet recital. She danced so beautifully. I was so impressed.\"\n"
     ]
    }
   ],
   "source": [
    "len_of_db = len(starting_exchange_db)\n",
    "index_of_sample = random.randint(0, len_of_db)\n",
    "sample_text = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "sample_text = re.sub(\"_comma_\", ',', sample_text)\n",
    "print(f'Prompt: \"{sample_text}\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c2edf-6f8b-439c-983f-2c4dce6b04fa",
   "metadata": {},
   "source": [
    "### Response\n",
    "\n",
    "We provide a response to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "963666b4-3915-4a26-b780-b3d98d972397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide your response:  Yay, you should feel so proud of her\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "while(flag):\n",
    "    answer = input(\"Provide your response: \")\n",
    "    if answer.lower() == '':\n",
    "        print('No answer received, please provide a response')\n",
    "    else:\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de813-f386-40e2-8e56-83395a83517f",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "In this cell, the prompt-response pair is processed to have the format and features required for the classification algorithm. \n",
    "\n",
    "Subsequently, the data is passed to the classifier, and a prediction it made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "427849ee-3c1d-4775-80b9-3a59b8e70732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data....\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classification algorithm predicts a level of 3 out of 3 for the perceived empathy of your response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "df = starting_exchange_db.iloc[[index_of_sample]]\n",
    "df = df.reset_index(drop=True)\n",
    "columns_list = starting_exchange_db.columns.to_list()\n",
    "df.loc[0, 'listener_utterance'] = str(answer)\n",
    "C = list(set(columns_list) - set(['speaker_utterance','listener_utterance','empathy','exchange_number']))\n",
    "df = df.drop(columns = C)\n",
    "df = process_answer(df,feature_vector)\n",
    "df = df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "x_test = df.drop(columns=['empathy'])\n",
    "y_test = df.drop(columns=x_test.columns)\n",
    "y_pred = pbc.predict(x_test)\n",
    "print(f'Our classification algorithm predicts a level of {int(y_pred[0]) + 1} out of 3 for the perceived empathy of your response')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5623fa0-bded-4fc9-8da9-89155d0379be",
   "metadata": {},
   "source": [
    "### Multi-turn inference\n",
    "\n",
    "In this cell, we run multiple inferences through utterance exchanges. Together, these exchanges form a conversation centered in an emotional topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ce614472-749f-4913-a783-b9d579c17d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data....\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(columns_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistener_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempathy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexchange_number\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     23\u001b[0m conv_df \u001b[38;5;241m=\u001b[39m conv_df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m C)\n\u001b[0;32m---> 24\u001b[0m conv_df \u001b[38;5;241m=\u001b[39m process_answer(conv_df\u001b[38;5;241m.\u001b[39miloc[[\u001b[38;5;241m1\u001b[39m]], feature_vector)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(conv_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     26\u001b[0m conv_df \u001b[38;5;241m=\u001b[39m conv_df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistener_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[167], line 10\u001b[0m, in \u001b[0;36mprocess_answer\u001b[0;34m(sample_df, control_vector)\u001b[0m\n\u001b[1;32m      8\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m sample_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistener_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m control_vector[feature2number[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepitome\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     sample_df \u001b[38;5;241m=\u001b[39m epitome\u001b[38;5;241m.\u001b[39mclassify_epitome_values(epitome_empathy_classifier, sample_df)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m control_vector[feature2number[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAD_vectors\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     sample_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvad_speaker\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(lexicon\u001b[38;5;241m.\u001b[39mget_avg_vad, args \u001b[38;5;241m=\u001b[39m (lexicon_df,wnl,stp_wrds)) \n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/classifiers/epitome_mechanisms/epitome_predictor.py:36\u001b[0m, in \u001b[0;36mclassify_epitome_values\u001b[0;34m(empathy_classifier, in_df)\u001b[0m\n\u001b[1;32m     33\u001b[0m epitome_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions_EX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(epitome_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#(logits_empathy_ER, predictions_ER, logits_empathy_IP, predictions_IP, logits_empathy_EX, predictions_EX, logits_rationale_ER, predictions_rationale_ER, logits_rationale_IP, predictions_rationale_IP, logits_rationale_EX,predictions_rationale_EX) = empathy_classifier.predict_empathy([epitome_df.loc[i, 'seeker_post']], [epitome_df.loc[i, 'response_post']])\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     (logits_empathy_ER, predictions_ER, logits_empathy_IP, predictions_IP, logits_empathy_EX, predictions_EX, logits_rationale_ER, predictions_rationale_ER, logits_rationale_IP, predictions_rationale_IP, logits_rationale_EX,predictions_rationale_EX) \u001b[38;5;241m=\u001b[39m empathy_classifier\u001b[38;5;241m.\u001b[39mpredict_empathy([\u001b[38;5;28mstr\u001b[39m(epitome_df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m])], [\u001b[38;5;28mstr\u001b[39m(epitome_df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlistener_utterance\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#epitome_df.loc[i] = [ids[i], seeker_posts[i], response_posts[i], predictions_ER[0], predictions_IP[0], predictions_EX[0], predictions_rationale_ER[0].tolist(), predictions_rationale_IP[0].tolist(), predictions_rationale_EX[0].tolist()]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     epitome_df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions_ER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions_ER[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/frame.py:4012\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4006\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4009\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4010\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4012\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(index)\n\u001b[1;32m   4013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[1;32m   4015\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   4016\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_det/lib/python3.12/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "\n",
    "#1470 <---- index of a good conversation\n",
    "# hit:7872_conv:15745\n",
    "\n",
    "while(flag == 0):\n",
    "    len_of_db = len(starting_exchange_db)\n",
    "    index_of_sample = random.randint(0, len_of_db-1)\n",
    "    sample_conv_start = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "    conv_id = starting_exchange_db.loc[index_of_sample,'conv_id']\n",
    "    #print(conv_id)\n",
    "    #print(f'Prompt: \"{sample_text}\"') \n",
    "    \n",
    "    full_database = pd.read_csv(current_dir + '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv')\n",
    "    conv_df = full_database[full_database['conv_id'] == conv_id]\n",
    "    conv_df = conv_df.reset_index(drop=True)\n",
    "    if len(conv_df) > 2 and conv_df.loc[0,'empathy'] == 3:\n",
    "        flag = 1\n",
    "    else:\n",
    "        flag = 0\n",
    "\n",
    "C = list(set(columns_list) - set(['speaker_utterance','listener_utterance','empathy','exchange_number']))\n",
    "conv_df = conv_df.drop(columns = C)\n",
    "conv_df = process_answer(conv_df.iloc[[1]].reset_index(drop=True), feature_vector)\n",
    "print(conv_df.head())\n",
    "conv_df = conv_df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "x_test = conv_df.drop(columns=['empathy'])\n",
    "y_test = conv_df.drop(columns=x_test.columns)\n",
    "y_pred = pbc.predict(x_test)\n",
    "for x in y_pred:\n",
    "    print(x + 1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83267db2-c137-4f18-a596-655861fcf154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
