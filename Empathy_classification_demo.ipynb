{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d0a22c-4c57-42ec-9fd1-acccab9a2cc7",
   "metadata": {},
   "source": [
    "# DEMO: Empathy classification using a pattern classifier\n",
    "\n",
    "In this notebook, it is possible to use a previously trained contrast-pattern classification algorithm to obtain the empathy level of a conversation between two people. \n",
    "\n",
    "A conversation prompt is presented, pulled from the EmpatheticExchanges database subset for testing classification algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c48750-894d-4cfa-b690-743951e0d5e6",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "This subsection focuses on setting up the environment, functions, utilities, and models required for the demo. Likewise, it is where the variables are manually declared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d243c2-35f2-4a20-a573-6142fc14b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "#import classifier\n",
    "from PBC4cip import PBC4cip\n",
    "from PBC4cip.core.Evaluation import obtainAUCMulticlass\n",
    "from PBC4cip.core.Helpers import get_col_dist, get_idx_val\n",
    "\n",
    "#utilities for database management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import train_classifier as trainer\n",
    "import test_classifier as tester\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "#relevant classifiers for annotating exchange feature\n",
    "from classifiers.empathetic_intent import intent_prediction as ip\n",
    "from classifiers.sentiment import sentiment_prediction as sp\n",
    "from classifiers.epitome_mechanisms import epitome_predictor as epitome\n",
    "from classifiers.nrc_vad_lexicon import lexicon_analysis as lexicon\n",
    "from classifiers.course_grained_emotion import pretrained_32emotions as em32\n",
    "from classifiers.course_grained_emotion import emotion_reductor as em_red\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "from spellchecker import SpellChecker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ab96-e4aa-47e5-b4f4-5face72f158e",
   "metadata": {},
   "source": [
    "### Selection of features\n",
    "\n",
    "In this cell, we define the model that will be used for this task. We declare its location directory, and its name \"trained_pbc4cip.sav\" \n",
    "\n",
    "Likewise, we declare a \"feature vector\" which contains binary flags for the features used by the model to predict empathy. \n",
    "\n",
    "Finally, we declare the database from which the prompts will be extracted from by declaring its directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aab462-ebf7-495c-ba30-168b3f718f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant directories\n",
    "current_dir = os.getcwd() #get directory of the repository\n",
    "#Select an appropriate classification model in the Experiments folder\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "\n",
    "feature2number = {'database_to_classify':0,'intent' : 1, 'sentiment' : 2, 'epitome':3, 'VAD_vectors':4, 'utterance_length':5,\n",
    "                  '32_emotion_labels':6,'20_emotion_labels':7, \n",
    "                  '8_emotion_labels':8, 'emotion_mimicry':9, 'Reduce_empathy_labels':10, \n",
    "                  'exchange_number' : 11}\n",
    "\n",
    "feature_vector = [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
    "'''\n",
    "                 [1,#database to pull from 0 = empatheticconversations (old), 1 empatheticexchanges (new)\n",
    "                  1,#intent\n",
    "                  1,#sentiment\n",
    "                  0,#epitome\n",
    "                  1,#vad lexicon\n",
    "                  1,#length\n",
    "                  0,#emotion 32\n",
    "                  0,#emotion 20\n",
    "                  1,#emotion 8\n",
    "                  1,#emotion mimicry\n",
    "                  1, #reduce empathy labels\n",
    "                  1 #exchange number\n",
    "                  ]\n",
    "'''\n",
    "\n",
    "if feature_vector[feature2number['database_to_classify']] == 1: \n",
    "    database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges_test.csv'\n",
    "else: \n",
    "    database_dir = '/processed_databases/EmpatheticConversationsExchangeFormat/EmpatheticConversations_ex.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548e12f-e577-4f23-b177-d4273c9def5b",
   "metadata": {},
   "source": [
    "### Loading classification models\n",
    "\n",
    "In this cell, we prepare the classification models for obtaining empathy-related features. These models must be pretrained before they are loaded by this demo. \n",
    "\n",
    "### WARNING: DO NOT RUN THIS TWICE. It will cause memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6768f2-4db7-4bb2-87b7-d0b7b391a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load intent model\n",
    "if feature_vector[feature2number['intent']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    model_intent,tokenizer_intent,device = ip.loadModelTokenizerAndDevice(empIntSubDir) #get model and parameters\n",
    "#load sentiment model\n",
    "if feature_vector[feature2number['sentiment']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    sent_model, sent_tokenzr = sp.loadSentimentModel() #get model and tokenizer\n",
    "#epitome model is loaded during inference due to the code of its classifier\n",
    "if feature_vector[feature2number['epitome']] == 1:\n",
    "    epitome_empathy_classifier = epitome.load_epitome_classifier('classifiers/epitome_mechanisms/trained_models')\n",
    "#load lexicon\n",
    "if feature_vector[feature2number['VAD_vectors']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')\n",
    "#load emotion classifier with 32 labels for any of the emotion labels options\n",
    "if (feature_vector[feature2number['32_emotion_labels']] == 1) or (feature_vector[feature2number['20_emotion_labels']] == 1) or (feature_vector[feature2number['8_emotion_labels']] == 1):\n",
    "    emo32_model, emo32_tokenzr = em32.load32EmotionsModel() #get model and tokenizer\n",
    "#it is necessary to get the VAD vectors for obtaining emotion mimicry\n",
    "if feature_vector[feature2number['emotion_mimicry']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08573e51-15fc-4461-b8a6-65f7ca8a13a7",
   "metadata": {},
   "source": [
    "### Definition of data processing function\n",
    "\n",
    "This is a function used to transform a text exchange into the format necessary for classification. It adds the following features: \n",
    "\n",
    "* Sentiment\n",
    "* EPITOME mechanisms (Sharma, 2019)\n",
    "* Valence, Arousal, and Dominance emotion vectors\n",
    "* Utterance lengths for both participants\n",
    "* Emotion labels\n",
    "* Empathetic Intent\n",
    "* Whether there is emotion mimicry\n",
    "\n",
    "This features are dependent on the feature vector defined at the start of this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ab89d-2d9d-433e-92d4-43c7f1de60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(sample_df,control_vector):\n",
    "    print('processing data....')\n",
    "    if control_vector[feature2number['sentiment']] == 1: \n",
    "        sample_df['speaker_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'speaker_utterance')) \n",
    "        sample_df[['s_negative','s_neutral', 's_positive']] = pd.DataFrame(sample_df.speaker_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df['listener_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'listener_utterance')) \n",
    "        sample_df[['l_negative','l_neutral', 'l_positive']] = pd.DataFrame(sample_df.listener_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['speaker_sentiment','listener_sentiment'])\n",
    "    if control_vector[feature2number['epitome']] == 1:\n",
    "        sample_df = epitome.classify_epitome_values(epitome_empathy_classifier, sample_df)\n",
    "    if control_vector[feature2number['VAD_vectors']] == 1:\n",
    "        sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "        sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])\n",
    "    if control_vector[feature2number['utterance_length']] == 1:\n",
    "        sample_df['s_word_len'] = sample_df['speaker_utterance'].apply(data_processer.get_word_len) \n",
    "        sample_df['l_word_len'] = sample_df['listener_utterance'].apply(data_processer.get_word_len) \n",
    "    if (control_vector[feature2number['32_emotion_labels']] == 1) or (control_vector[feature2number['20_emotion_labels']] == 1) or (control_vector[feature2number['8_emotion_labels']] == 1):\n",
    "        sample_df['speaker_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'speaker_utterance')) \n",
    "        sample_df['listener_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'listener_utterance')) \n",
    "        if (control_vector[feature2number['20_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels('listener_emotion',sample_df)\n",
    "        if (control_vector[feature2number['8_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('listener_emotion',sample_df)\n",
    "    if control_vector[feature2number['intent']] == 1: \n",
    "        sample_df['utterance'] = str(answer)\n",
    "        sample_df['is_response'] = 1\n",
    "        sample_df['empathetic_intent'] = sample_df.apply(data_processer.get_emp_intent_probabilities, axis=1, args = (model_intent,tokenizer_intent,device,'listener_utterance'))\n",
    "        sample_df[data_processer.intent_labels] = pd.DataFrame(sample_df.empathetic_intent.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['empathetic_intent','utterance','is_response'])\n",
    "    if control_vector[feature2number['emotion_mimicry']] == 1:\n",
    "        if(control_vector[4] == 1):\n",
    "            #get the emotional similarity, if it is more than 0.7 set mimicry to 1\n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns = ['emotional_similarity'])\n",
    "        else: \n",
    "            sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds,spll)) \n",
    "            sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds,spll)) \n",
    "            sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "            sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "            sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])                \n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns =  ['valence_speaker','arousal_speaker','dominance_speaker','valence_listener','arousal_listener','dominance_listener','emotional_similarity'])\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('category')\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('string')\n",
    "        #sample_df = sample_df.drop(columns =  ['predictions_EX'])\n",
    "    print('done')\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53078ccd-bfa0-4c1c-8a7d-b9c4572d2008",
   "metadata": {},
   "source": [
    "### database setup\n",
    "\n",
    "We load the database. Next, we filter it to have only samples that start a conversation. This is done by selecting those that have an \"exchange_number\" variable of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5739e968-0026-46a3-80cf-8d9ec1ed07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(current_dir + database_dir)\n",
    "\n",
    "starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "starting_exchange_db = starting_exchange_db.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78d7e6-b49d-4268-9b9f-41f665cb382e",
   "metadata": {},
   "source": [
    "### Load our classification model\n",
    "\n",
    "In this cell, we run the empathy classification model that we have previously trained. The model selection is done through specifying the directory in which the model was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec3862b-594f-4d71-bbf2-c3fc7c672abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "pbc = pickle.load(open(model_directory, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4fd79-91cd-47cf-bfc8-8aac6c569646",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "In this subsection, we present that working parts of the demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8509-f8a2-49ba-99ba-5ae0b971e6a4",
   "metadata": {},
   "source": [
    "### Conversation starter\n",
    "\n",
    "We randomly sample the database for a conversation prompt. This is equivalent to an utterance of a first agent, to which we will provide a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb94567e-25c0-4522-8f1e-a68816831299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"My girlfriend always tells me what to do.\"\n"
     ]
    }
   ],
   "source": [
    "len_of_db = len(starting_exchange_db)\n",
    "index_of_sample = random.randint(0, len_of_db)\n",
    "sample_text = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "sample_text = re.sub(\"_comma_\", ',', sample_text)\n",
    "print(f'Prompt: \"{sample_text}\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c2edf-6f8b-439c-983f-2c4dce6b04fa",
   "metadata": {},
   "source": [
    "### Response\n",
    "\n",
    "We provide a response to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963666b4-3915-4a26-b780-b3d98d972397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide your response:  Oh, well, if you are both ok with that, there is no problem\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "while(flag):\n",
    "    answer = input(\"Provide your response: \")\n",
    "    if answer.lower() == '':\n",
    "        print('No answer received, please provide a response')\n",
    "    else:\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de813-f386-40e2-8e56-83395a83517f",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "In this cell, the prompt-response pair is processed to have the format and features required for the classification algorithm. \n",
    "\n",
    "Subsequently, the data is passed to the classifier, and a prediction it made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427849ee-3c1d-4775-80b9-3a59b8e70732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data....\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classification algorithm predicts a level of 1 out of 3 for the perceived empathy of your response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "df = starting_exchange_db.iloc[[index_of_sample]]\n",
    "df = df.reset_index(drop=True)\n",
    "columns_list = starting_exchange_db.columns.to_list()\n",
    "df.loc[0, 'listener_utterance'] = str(answer)\n",
    "C = list(set(columns_list) - set(['speaker_utterance','listener_utterance','empathy','exchange_number']))\n",
    "df = df.drop(columns = C)\n",
    "df = process_answer(df,feature_vector)\n",
    "df = df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "x_test = df.drop(columns=['empathy'])\n",
    "y_test = df.drop(columns=x_test.columns)\n",
    "y_pred = pbc.predict(x_test)\n",
    "print(f'Our classification algorithm predicts a level of {int(y_pred[0]) + 1} out of 3 for the perceived empathy of your response')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5623fa0-bded-4fc9-8da9-89155d0379be",
   "metadata": {},
   "source": [
    "### Multi-turn inference\n",
    "\n",
    "In this cell, we run multiple inferences through utterance exchanges. Together, these exchanges form a conversation centered in an emotional topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce614472-749f-4913-a783-b9d579c17d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of an empathetic multi-turn conversation\n",
      "Turn 1\n",
      "Agent A: I have an old truck that I enjoy working on and driving around.\n",
      "Agent B: What great fun! My husband had a 1960 Ford F150 he completely redid. It was quite an attraction around town!\n",
      "Turn 2\n",
      "Agent A: That's cool. Mine is a 68 chevy c10\n",
      "Agent B: Nice! Where did you find it?\n",
      "Turn 3\n",
      "Agent A: It's a family heirloom.\n",
      "Agent B: Even better. Are you doing all the work yourself or do you have a restoration person helping you?\n",
      "Turn 4\n",
      "Agent A: I've done it all myself. It takes a long time_comma_ but that's part of the fun.\n",
      "Agent B: That's true. You must be a very good mechanic to do all that - I know the amount of work it takes!\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "\n",
    "#1470 <---- index of a good conversation\n",
    "# hit:7872_conv:15745\n",
    "\n",
    "while(flag == 0):\n",
    "    len_of_db = len(starting_exchange_db)\n",
    "    index_of_sample = random.randint(0, len_of_db-1)\n",
    "    sample_conv_start = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "    conv_id = starting_exchange_db.loc[index_of_sample,'conv_id']\n",
    "    #print(conv_id)\n",
    "    #print(f'Prompt: \"{sample_text}\"') \n",
    "    \n",
    "    full_database = pd.read_csv(current_dir + '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv')\n",
    "    conv_df = full_database[full_database['conv_id'] == conv_id]\n",
    "    conv_df = conv_df.reset_index(drop=True)\n",
    "    if len(conv_df) > 2 and conv_df.loc[0,'empathy'] > 2:\n",
    "        flag = 1\n",
    "    else:\n",
    "        flag = 0\n",
    "print('Example of an empathetic multi-turn conversation')\n",
    "for i in range(len(conv_df)):\n",
    "    print(f'Turn {conv_df.loc[i,'exchange_number']}')\n",
    "    print(f'Agent A: {conv_df.loc[i,'speaker_utterance']}')\n",
    "    print(f'Agent B: {conv_df.loc[i,'listener_utterance']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83267db2-c137-4f18-a596-655861fcf154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide an uttereance by Agent A:  end\n"
     ]
    }
   ],
   "source": [
    "mt_df = pd.DataFrame()\n",
    "for i in range(len(conv_df)):\n",
    "    query = input(\"Provide an uttereance by Agent A: \")\n",
    "    if ('end' in query) or ('quit' in query):\n",
    "        break\n",
    "    answer = input(\"Provide an uttereance by Agent B: \")\n",
    "    if ('end' in aswer) or ('quit' in aswer):\n",
    "        break\n",
    "    datarow = {'speaker_utterance': [str(query)], 'listener_utterance': [str(answer)],'exchange_number': [i+1], 'empathy': [3]}\n",
    "    mt_df = pd.concat([mt_df,pd.DataFrame.from_dict(datarow)])\n",
    "    ex_df = process_answer(mt_df.iloc[[i]].reset_index(drop=True), feature_vector)\n",
    "    ex_df = ex_df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "    x_test = ex_df.drop(columns=['empathy'])\n",
    "    y_test = ex_df.drop(columns=x_test.columns)\n",
    "    y_pred = pbc.predict(x_test)\n",
    "    print(f'Predicted empathy of the exchange: {y_pred[0]+1}/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fbf4d-ebe5-4d69-b285-2ce1a2f1f1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36661ffe-c91a-4f73-916e-3af534b437b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
