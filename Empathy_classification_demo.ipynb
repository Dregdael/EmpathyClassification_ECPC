{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d0a22c-4c57-42ec-9fd1-acccab9a2cc7",
   "metadata": {},
   "source": [
    "# DEMO: Empathy classification using a pattern classifier\n",
    "\n",
    "In this notebook, it is possible to use a previously trained contrast-pattern classification algorithm to obtain the empathy level of a conversation between two people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d243c2-35f2-4a20-a573-6142fc14b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "#import classifier\n",
    "from PBC4cip import PBC4cip\n",
    "from PBC4cip.core.Evaluation import obtainAUCMulticlass\n",
    "from PBC4cip.core.Helpers import get_col_dist, get_idx_val\n",
    "\n",
    "#utilities for database management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import train_classifier as trainer\n",
    "import test_classifier as tester\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "#relevant classifiers for annotating exchange feature\n",
    "from classifiers.empathetic_intent import intent_prediction as ip\n",
    "from classifiers.sentiment import sentiment_prediction as sp\n",
    "from classifiers.epitome_mechanisms import epitome_predictor as epitome\n",
    "from classifiers.nrc_vad_lexicon import lexicon_analysis as lexicon\n",
    "from classifiers.course_grained_emotion import pretrained_32emotions as em32\n",
    "from classifiers.course_grained_emotion import emotion_reductor as em_red\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "from spellchecker import SpellChecker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ab96-e4aa-47e5-b4f4-5face72f158e",
   "metadata": {},
   "source": [
    "## Loading utilities and models\n",
    "\n",
    "First, we will obtain the model, and load all utilities necessary for obtaining the features for a conversation exchange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74aab462-ebf7-495c-ba30-168b3f718f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant directories\n",
    "current_dir = os.getcwd() #get directory of the repository\n",
    "#Select an appropriate classification model in the Experiments folder\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "\n",
    "feature2number = {'database_to_classify':0,'intent' : 1, 'sentiment' : 2, 'epitome':3, 'VAD_vectors':4, 'utterance_length':5,\n",
    "                  '32_emotion_labels':6,'20_emotion_labels':7, \n",
    "                  '8_emotion_labels':8, 'emotion_mimicry':9, 'Reduce_empathy_labels':10, \n",
    "                  'exchange_number' : 11}\n",
    "\n",
    "feature_vector = [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
    "'''\n",
    "                 [1,#database to pull from 0 = empatheticconversations (old), 1 empatheticexchanges (new)\n",
    "                  1,#intent\n",
    "                  1,#sentiment\n",
    "                  0,#epitome\n",
    "                  1,#vad lexicon\n",
    "                  1,#length\n",
    "                  0,#emotion 32\n",
    "                  0,#emotion 20\n",
    "                  1,#emotion 8\n",
    "                  1,#emotion mimicry\n",
    "                  1, #reduce empathy labels\n",
    "                  1 #exchange number\n",
    "                  ]\n",
    "'''\n",
    "\n",
    "if feature_vector[feature2number['database_to_classify']] == 1: \n",
    "    database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges_test.csv'\n",
    "else: \n",
    "    database_dir = '/processed_databases/EmpatheticConversationsExchangeFormat/EmpatheticConversations_ex.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a2ab89d-2d9d-433e-92d4-43c7f1de60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load intent model\n",
    "if feature_vector[feature2number['intent']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    model_intent,tokenizer_intent,device = ip.loadModelTokenizerAndDevice(empIntSubDir) #get model and parameters\n",
    "#load sentiment model\n",
    "if feature_vector[feature2number['sentiment']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    sent_model, sent_tokenzr = sp.loadSentimentModel() #get model and tokenizer\n",
    "#epitome model is loaded during inference due to the code of its classifier\n",
    "#load lexicon\n",
    "if feature_vector[feature2number['VAD_vectors']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')\n",
    "#load emotion classifier with 32 labels for any of the emotion labels options\n",
    "if (feature_vector[feature2number['32_emotion_labels']] == 1) or (feature_vector[feature2number['20_emotion_labels']] == 1) or (feature_vector[feature2number['8_emotion_labels']] == 1):\n",
    "    emo32_model, emo32_tokenzr = em32.load32EmotionsModel() #get model and tokenizer\n",
    "#it is necessary to get the VAD vectors for obtaining emotion mimicry\n",
    "if feature_vector[feature2number['emotion_mimicry']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')\n",
    "\n",
    "\n",
    "def process_answer(sample_df,control_vector):\n",
    "    print('processing data....')\n",
    "    columns_2_keep = []\n",
    "    if control_vector[feature2number['intent']] == 1: \n",
    "        sample_df['utterance'] = str(answer)\n",
    "        sample_df['is_response'] = 1\n",
    "        sample_df['empathetic_intent'] = sample_df.apply(data_processer.get_emp_intent_probabilities, axis=1, args = (model_intent,tokenizer_intent,device,'utterance'))\n",
    "        sample_df[data_processer.intent_labels] = pd.DataFrame(sample_df.empathetic_intent.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['empathetic_intent','utterance','is_response'])\n",
    "    if control_vector[feature2number['sentiment']] == 1: \n",
    "        sample_df['speaker_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'speaker_utterance')) #apply sentiment label extraction to speaker\n",
    "        sample_df[['s_negative','s_neutral', 's_positive']] = pd.DataFrame(sample_df.speaker_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df['listener_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'listener_utterance')) #apply sentiment label extraction to speaker\n",
    "        sample_df[['l_negative','l_neutral', 'l_positive']] = pd.DataFrame(sample_df.listener_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['speaker_sentiment','listener_sentiment'])\n",
    "    if control_vector[feature2number['epitome']] == 1:\n",
    "        sample_df = epitome.predict_epitome_values('classifiers/epitome_mechanisms/trained_models',sample_df)\n",
    "    if control_vector[feature2number['VAD_vectors']] == 1:\n",
    "        sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "        sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])\n",
    "        #columns_2_keep += ['valence_speaker','arousal_speaker','dominance_speaker','valence_listener','arousal_listener','dominance_listener']\n",
    "    if control_vector[feature2number['utterance_length']] == 1:\n",
    "        sample_df['s_word_len'] = sample_df['speaker_utterance'].apply(data_processer.get_word_len) \n",
    "        sample_df['l_word_len'] = sample_df['listener_utterance'].apply(data_processer.get_word_len) \n",
    "    if (control_vector[feature2number['32_emotion_labels']] == 1) or (control_vector[feature2number['20_emotion_labels']] == 1) or (control_vector[feature2number['8_emotion_labels']] == 1):\n",
    "        sample_df['speaker_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'speaker_utterance')) \n",
    "        sample_df['listener_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'listener_utterance')) \n",
    "        if (control_vector[feature2number['20_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels('listener_emotion',sample_df)\n",
    "        if (control_vector[feature2number['8_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('listener_emotion',sample_df)\n",
    "    if control_vector[feature2number['emotion_mimicry']] == 1:\n",
    "        if(control_vector[4] == 1):\n",
    "            #get the emotional similarity, if it is more than 0.7 set mimicry to 1\n",
    "            print('No labels detected, obtaining mimicry through emotional distance using VAD....')\n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) #obtain cosine similarity between valence and arousal vector\n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns = ['emotional_similarity'])\n",
    "        else: \n",
    "            #print('Annotating VAD values.....')\n",
    "            sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "            sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "            sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "            sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "            sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])                \n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) #obtain cosine similarity between valence and arousal vector\n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns =  ['valence_speaker','arousal_speaker','dominance_speaker','valence_listener','arousal_listener','dominance_listener','emotional_similarity'])\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('category')\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('string')\n",
    "    print('done')\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53078ccd-bfa0-4c1c-8a7d-b9c4572d2008",
   "metadata": {},
   "source": [
    "## Loading database\n",
    "\n",
    "Next we load load the database and get a random sample of a conversation starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5739e968-0026-46a3-80cf-8d9ec1ed07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_utterance</th>\n",
       "      <th>listener_utterance</th>\n",
       "      <th>exchange_number</th>\n",
       "      <th>s_negative</th>\n",
       "      <th>s_neutral</th>\n",
       "      <th>s_positive</th>\n",
       "      <th>l_negative</th>\n",
       "      <th>...</th>\n",
       "      <th>acknowledging</th>\n",
       "      <th>encouraging</th>\n",
       "      <th>consoling</th>\n",
       "      <th>sympathizing</th>\n",
       "      <th>suggesting</th>\n",
       "      <th>questioning</th>\n",
       "      <th>wishing</th>\n",
       "      <th>neutral</th>\n",
       "      <th>mimicry</th>\n",
       "      <th>empathy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:10687_conv:21375</td>\n",
       "      <td>joyful</td>\n",
       "      <td>I was so happy when my mom came to visit me!</td>\n",
       "      <td>I was so happy when my mom came to visit me</td>\n",
       "      <td>Nice! How long has it been since you last saw ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.991476</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.998566</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:1876_conv:3752</td>\n",
       "      <td>confident</td>\n",
       "      <td>I felt confident when I finished my job interv...</td>\n",
       "      <td>I felt confident when I finished my job interv...</td>\n",
       "      <td>did y0u get it ?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.047845</td>\n",
       "      <td>0.950448</td>\n",
       "      <td>0.162308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:88_conv:176</td>\n",
       "      <td>afraid</td>\n",
       "      <td>I think my job will lay us off.</td>\n",
       "      <td>I think our job might be laying us off.</td>\n",
       "      <td>O No! Are they going to offer you any kind of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668925</td>\n",
       "      <td>0.319458</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.507649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.998379</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:8687_conv:17374</td>\n",
       "      <td>anxious</td>\n",
       "      <td>I'm not really sure if I am going to be able t...</td>\n",
       "      <td>I'm not really sure if I am going to be able t...</td>\n",
       "      <td>Why not?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511960</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.052616</td>\n",
       "      <td>0.252442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:4346_conv:8693</td>\n",
       "      <td>confident</td>\n",
       "      <td>I am playing an official gig with a band I'm i...</td>\n",
       "      <td>I am playing an official gig with a band I'm i...</td>\n",
       "      <td>Wow congrats! What type of music do you play?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.991831</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.996931</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>hit:10410_conv:20820</td>\n",
       "      <td>terrified</td>\n",
       "      <td>I heard some noises outside the house. I thoug...</td>\n",
       "      <td>I heard some noises outside the house. I thoug...</td>\n",
       "      <td>what</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462025</td>\n",
       "      <td>0.509884</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.351025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.972462</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>hit:10276_conv:20553</td>\n",
       "      <td>anticipating</td>\n",
       "      <td>I am looking forward to my wedding date!</td>\n",
       "      <td>I am looking forward to my wedding date! Of co...</td>\n",
       "      <td>Oh that is definitely something to be excited ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.991938</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>hit:4303_conv:8607</td>\n",
       "      <td>guilty</td>\n",
       "      <td>I yelled at my two-year-old nephew the other d...</td>\n",
       "      <td>I yelled at my two-year-old nephew the other d...</td>\n",
       "      <td>I think it's an adjustment you have to work th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956855</td>\n",
       "      <td>0.038004</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.489524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268683</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.720282</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>hit:11415_conv:22830</td>\n",
       "      <td>apprehensive</td>\n",
       "      <td>When we were deciding to move across the count...</td>\n",
       "      <td>I was really nervous to move across country.</td>\n",
       "      <td>why were you so nervous</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750556</td>\n",
       "      <td>0.228983</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>0.555310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.994134</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>hit:8210_conv:16420</td>\n",
       "      <td>guilty</td>\n",
       "      <td>I was supposed to study for a big test in coll...</td>\n",
       "      <td>I just had a huge test that I was supposed to ...</td>\n",
       "      <td>Oh no what happened? Why don't you think you d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.512657</td>\n",
       "      <td>0.376192</td>\n",
       "      <td>0.111152</td>\n",
       "      <td>0.679578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.998029</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  conv_id       context  \\\n",
       "0    hit:10687_conv:21375        joyful   \n",
       "1      hit:1876_conv:3752     confident   \n",
       "2         hit:88_conv:176        afraid   \n",
       "3     hit:8687_conv:17374       anxious   \n",
       "4      hit:4346_conv:8693     confident   \n",
       "..                    ...           ...   \n",
       "458  hit:10410_conv:20820     terrified   \n",
       "459  hit:10276_conv:20553  anticipating   \n",
       "460    hit:4303_conv:8607        guilty   \n",
       "461  hit:11415_conv:22830  apprehensive   \n",
       "462   hit:8210_conv:16420        guilty   \n",
       "\n",
       "                                                prompt  \\\n",
       "0         I was so happy when my mom came to visit me!   \n",
       "1    I felt confident when I finished my job interv...   \n",
       "2                      I think my job will lay us off.   \n",
       "3    I'm not really sure if I am going to be able t...   \n",
       "4    I am playing an official gig with a band I'm i...   \n",
       "..                                                 ...   \n",
       "458  I heard some noises outside the house. I thoug...   \n",
       "459           I am looking forward to my wedding date!   \n",
       "460  I yelled at my two-year-old nephew the other d...   \n",
       "461  When we were deciding to move across the count...   \n",
       "462  I was supposed to study for a big test in coll...   \n",
       "\n",
       "                                     speaker_utterance  \\\n",
       "0          I was so happy when my mom came to visit me   \n",
       "1    I felt confident when I finished my job interv...   \n",
       "2              I think our job might be laying us off.   \n",
       "3    I'm not really sure if I am going to be able t...   \n",
       "4    I am playing an official gig with a band I'm i...   \n",
       "..                                                 ...   \n",
       "458  I heard some noises outside the house. I thoug...   \n",
       "459  I am looking forward to my wedding date! Of co...   \n",
       "460  I yelled at my two-year-old nephew the other d...   \n",
       "461       I was really nervous to move across country.   \n",
       "462  I just had a huge test that I was supposed to ...   \n",
       "\n",
       "                                    listener_utterance  exchange_number  \\\n",
       "0    Nice! How long has it been since you last saw ...                1   \n",
       "1                                     did y0u get it ?                1   \n",
       "2    O No! Are they going to offer you any kind of ...                1   \n",
       "3                                             Why not?                1   \n",
       "4        Wow congrats! What type of music do you play?                1   \n",
       "..                                                 ...              ...   \n",
       "458                                               what                1   \n",
       "459  Oh that is definitely something to be excited ...                1   \n",
       "460  I think it's an adjustment you have to work th...                1   \n",
       "461                            why were you so nervous                1   \n",
       "462  Oh no what happened? Why don't you think you d...                1   \n",
       "\n",
       "     s_negative  s_neutral  s_positive  l_negative  ...  acknowledging  \\\n",
       "0      0.001461   0.007063    0.991476    0.004031  ...       0.000768   \n",
       "1      0.001707   0.047845    0.950448    0.162308  ...       0.000078   \n",
       "2      0.668925   0.319458    0.011617    0.507649  ...       0.000041   \n",
       "3      0.511960   0.435424    0.052616    0.252442  ...       0.000035   \n",
       "4      0.001250   0.006920    0.991831    0.000848  ...       0.000102   \n",
       "..          ...        ...         ...         ...  ...            ...   \n",
       "458    0.462025   0.509884    0.028091    0.351025  ...       0.000229   \n",
       "459    0.000774   0.007288    0.991938    0.001177  ...       0.004093   \n",
       "460    0.956855   0.038004    0.005142    0.489524  ...       0.268683   \n",
       "461    0.750556   0.228983    0.020461    0.555310  ...       0.000892   \n",
       "462    0.512657   0.376192    0.111152    0.679578  ...       0.000040   \n",
       "\n",
       "     encouraging  consoling  sympathizing  suggesting  questioning   wishing  \\\n",
       "0       0.000021   0.000023      0.000214    0.000168     0.998566  0.000094   \n",
       "1       0.000039   0.000015      0.000080    0.000176     0.999199  0.000088   \n",
       "2       0.000029   0.000023      0.000203    0.000817     0.998379  0.000254   \n",
       "3       0.000023   0.000047      0.000309    0.000683     0.998611  0.000145   \n",
       "4       0.000031   0.000023      0.001210    0.001222     0.996931  0.000144   \n",
       "..           ...        ...           ...         ...          ...       ...   \n",
       "458     0.000044   0.000024      0.000432    0.024983     0.972462  0.000456   \n",
       "459     0.001286   0.000084      0.000022    0.000238     0.000267  0.000287   \n",
       "460     0.000206   0.001094      0.005618    0.002770     0.000403  0.000271   \n",
       "461     0.000019   0.000045      0.000227    0.000109     0.994134  0.000171   \n",
       "462     0.000024   0.000037      0.000316    0.001151     0.998029  0.000088   \n",
       "\n",
       "      neutral  mimicry  empathy  \n",
       "0    0.000084        1        3  \n",
       "1    0.000279        1        2  \n",
       "2    0.000190        1        3  \n",
       "3    0.000091        0        2  \n",
       "4    0.000224        1        2  \n",
       "..        ...      ...      ...  \n",
       "458  0.001138        0        3  \n",
       "459  0.000419        1        3  \n",
       "460  0.720282        1        3  \n",
       "461  0.004301        0        3  \n",
       "462  0.000260        0        3  \n",
       "\n",
       "[463 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_csv(current_dir + database_dir)\n",
    "\n",
    "starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "starting_exchange_db = starting_exchange_db.reset_index(drop = True)\n",
    "starting_exchange_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78d7e6-b49d-4268-9b9f-41f665cb382e",
   "metadata": {},
   "source": [
    "### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ec3862b-594f-4d71-bbf2-c3fc7c672abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbc = pickle.load(open(model_directory, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8509-f8a2-49ba-99ba-5ae0b971e6a4",
   "metadata": {},
   "source": [
    "### picking a random conversation starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb94567e-25c0-4522-8f1e-a68816831299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Yesterday I failed my physics exam.\"\n"
     ]
    }
   ],
   "source": [
    "len_of_db = len(starting_exchange_db)\n",
    "index_of_sample = random.randint(0, len_of_db)\n",
    "\n",
    "sample_text = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "\n",
    "sample_text = re.sub(\"_comma_\", ',', sample_text)\n",
    "\n",
    "print(f'Prompt: \"{sample_text}\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c2edf-6f8b-439c-983f-2c4dce6b04fa",
   "metadata": {},
   "source": [
    "### type a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "963666b4-3915-4a26-b780-b3d98d972397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Provide your response:  I love you I love you I love you you are my world\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "while(flag):\n",
    "    answer = input(\"Provide your response: \")\n",
    "    if answer.lower() == '':\n",
    "        print('No answer received, please provide a response')\n",
    "    else:\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de813-f386-40e2-8e56-83395a83517f",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "427849ee-3c1d-4775-80b9-3a59b8e70732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_id                                              hit:1498_conv:2997\n",
      "context                                                    disappointed\n",
      "prompt                I I failed my physics exam yesterday. I was so...\n",
      "speaker_utterance                   Yesterday I failed my physics exam.\n",
      "listener_utterance    I love you I love you I love you you are my world\n",
      "exchange_number                                                       1\n",
      "s_negative                                                      0.93711\n",
      "s_neutral                                                      0.058596\n",
      "s_positive                                                     0.004294\n",
      "l_negative                                                     0.608533\n",
      "l_neutral                                                      0.365874\n",
      "l_positive                                                     0.025593\n",
      "valence_speaker                                                  -0.021\n",
      "arousal_speaker                                                  -0.051\n",
      "dominance_speaker                                                 -0.17\n",
      "valence_listener                                                 0.2808\n",
      "arousal_listener                                                -0.2548\n",
      "dominance_listener                                              -0.0356\n",
      "s_word_len                                                            6\n",
      "l_word_len                                                           17\n",
      "agreeing                                                       0.000194\n",
      "acknowledging                                                  0.000235\n",
      "encouraging                                                     0.00005\n",
      "consoling                                                      0.000242\n",
      "sympathizing                                                   0.987474\n",
      "suggesting                                                     0.000431\n",
      "questioning                                                    0.011044\n",
      "wishing                                                        0.000265\n",
      "neutral                                                        0.000065\n",
      "mimicry                                                               0\n",
      "empathy                                                               3\n",
      "Name: 0, dtype: object\n",
      "processing data....\n",
      "No labels detected, obtaining mimicry through emotional distance using VAD....\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "exchange_number              1\n",
       "empathy                      3\n",
       "agreeing              0.001059\n",
       "acknowledging         0.000569\n",
       "encouraging           0.000108\n",
       "consoling             0.000007\n",
       "sympathizing          0.000215\n",
       "suggesting            0.000095\n",
       "questioning           0.000487\n",
       "wishing               0.001096\n",
       "neutral               0.996365\n",
       "s_negative             0.93711\n",
       "s_neutral             0.058596\n",
       "s_positive            0.004294\n",
       "l_negative            0.002831\n",
       "l_neutral             0.007368\n",
       "l_positive            0.989802\n",
       "predictions_ER               1\n",
       "predictions_IP               0\n",
       "predictions_EX               0\n",
       "valence_speaker         -0.021\n",
       "arousal_speaker         -0.051\n",
       "dominance_speaker        -0.17\n",
       "valence_listener         0.811\n",
       "arousal_listener       -0.0245\n",
       "dominance_listener      0.3845\n",
       "s_word_len                   6\n",
       "l_word_len                  13\n",
       "mimicry                      0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {'speaker_utterance': [sample_text], 'listener_utterance': [answer]}\n",
    "\n",
    "df = starting_exchange_db.iloc[[index_of_sample]]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "columns_list = starting_exchange_db.columns.to_list()\n",
    "df.loc[0, 'listener_utterance'] = str(answer)\n",
    "\n",
    "\n",
    "C = list(set(columns_list) - set(['speaker_utterance','listener_utterance','empathy','exchange_number']))\n",
    "\n",
    "print(df.iloc[0])\n",
    "\n",
    "df = df.drop(columns = C)\n",
    "df = process_answer(df,feature_vector)\n",
    "\n",
    "df.loc[0, 'listener_utterance']\n",
    "\n",
    "df = df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98f26b-96ed-42b6-a501-e84cc15e4828",
   "metadata": {},
   "source": [
    "### pass it to the model and predict value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c98b9fa-e056-4101-a86a-268fec4585e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = df.drop(columns=['empathy'])\n",
    "y_test = df.drop(columns=x_test.columns)\n",
    "y_pred = pbc.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e3b90-cc0c-4254-8c45-c8b4920541e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
